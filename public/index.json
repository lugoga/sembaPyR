[{"categories":[],"contents":"\rIf you work with data science, R and Python must be the two programming languages that you use the most. Both R and Python are quite robust languages and either one of them is actually sufficient to carry out the data analysis task. However, instead of considering them as tools that supplement each other, more often you will find people dealing with data claim one language to be better than the other. Truth be told, R and Python are excellent tools in ther own right but are often conceived as rivals. One major reason for such view lies on the experts. Because data analysts have divided the data science field into camps based on the choice of the programming language they are familiar with.\nThere major two camps—R camp and Python camp—and history is the testimony that camps can not livel in harmony. Members of both camps believe that their choice of language . Honestly, I do not hold to their opinion, but rather wish I have skills for both languages. So, whether you have in R or Python camp, one thing you will notice is that the problem we have in data science is simply that divergence does not lie with the tools but with the people using those tools.\nI believe there are few people in the Data Science community who use both R and Pythhon in their analytical workflow. But majority are committed to only one programming language, but wish they had access to some functions from other language. Therefore, there is no reason that hold us to stick using this programming language or the other. Our ultimate goal should be to do better analytics and derive better insights and choice of which programming language to use should not hinder us from reaching our goals.\nThe questions that always resolute in my mind is whether can we utilize the statistical power of R along with the programming capabilities of Python?. Its undeniable truth that there are definitely some high and low points for both languages and if we can utilize the strength of both, we can end up dong a much better job. Thanks to Kevin Ushey and his collegues [-@reticulate] for devloping a reticulate package. reticulate package provides a comprehensive set of tools that allows to work with R and Python in the same environment. The reticulate package provide the following facilities;\n\rCalling Python from R in a variety of ways including rmarkdown, sourcing, Python scripts, importing Python modules and using Python interactively within and R session.\rTranslation between R and Python objects—for example r_to_py function allows to construct R to Pandas data frame and py_to_r() function convert python object like data frame, matrix and etc to R\rFlexible binding to different versions of Python including virtual environments and conda environment.\r\rSetup Python in Rstudio\rTo make python work in Rstudio, we must make some setup. The first thing we need to load the reticulate package. Once the reticulate package in the workspace, we use the use_python(PATH) and define the path of Python or Anaconda in your local machine.\n\rNote that setting the Python enviroment and loading the reticulate package is done in the R chunk\n\rrequire(reticulate)\ruse_python(\u0026quot;c:/Python/Anaconda3/\u0026quot;)\rOnce we have configured the Python environment in R, we must also load some module from R into Rstudio. This will make the functions in these module avaiable for our processing and plotting. We use the convention naming of Python package as seen in the chunk below.\n\rNote that loading Python modules in R session must be done inside the Python chunk. Otherwise you get an error message or a chunk fail to iterate the process\n\rimport pandas as pd\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport seaborn as sns\r\rTibble to Pandas Dataframe\rIn this section we use function from R to read the dataset from the local machine into R session. The function readxl::read_excel() function is used. The outocome after reading the file is the tibble format—a modern data frame from the tidyverse ecosystem. We first load the tidyverse\nrequire(tidyverse)\rread the excel file with readxl and manipulate the data with dplyr to the write format\nkgr = readxl::read_excel(\u0026quot;c:/Users/Semba/Documents/kuguru.xlsx\u0026quot;)\rOnce the tibble file is in the environment, we need to convert from tibble data frame into pandas dataframe. Make a copy of pandas dataframe from tible with the r. function\n\rnote that conversion from tibble to pandas data frame must be done in the Python chunk and not R chunk\n\rkgr_df = kgr %\u0026gt;% mutate(month = lubridate::month(time, abbr = T, label = T))%\u0026gt;% select(sst,chl, month) \rThen construct pandas data frame from tibble\n##\rkgrpy = r.kgr_df\rWe then the power of seaborn package from Python to visualize. For this case, I used the function pairplot—a pairwise plot that show the relationships of variables present in pandas Dataframe. The chunk below was used to plot figure 1\n## make a pairplot with seaborn\rsns.pairplot(kgrpy, hue=\u0026quot;month\u0026quot;)\r\u0026lt;seaborn.axisgrid.PairGrid object at 0x000000002513AC48\u0026gt;\rplt.show()\r\rFigure 1: The relationship of variables in the Pandas Dataframe\r\rThe chunk below highlight the scatterplot of the relationship between SST and chlorophyll concentration plotted with scatterplot()function from seaborn module and the plot is represented in figure 2\nfig = plt.figure()\rsns.scatterplot(data=kgrpy, x=\u0026#39;chl\u0026#39;, y=\u0026#39;sst\u0026#39;, hue=\u0026#39;month\u0026#39;)\rplt.xlabel(\u0026quot;Concentration of Chlorophyll (mg/l)\u0026quot;)\rplt.ylabel(\u0026quot;Sea surface temperature\u0026quot;)\rplt.gca().legend_.remove()\rplt.show()\r\rFigure 2: Relationship between SST and Chlorophyll\r\r\rheatmaps with seaborn module\rHeatmaps are three dimensional plots, as previous, we have to import the data into workspace and manipulate it so that months use the label instead of numbers.\nkgr = readxl::read_excel(\u0026quot;c:/Users/Semba/Documents/kuguru.xlsx\u0026quot;)\rkgr_dt_df = kgr %\u0026gt;% mutate(month = lubridate::month(time, abbr = T, label = T))%\u0026gt;% select(sst,chl, month, year) \rSince we have used R chunk, to use seaborn Python module function, we must convert the data first to pandas Dataframe, which then allows us the capability of Python. We use the r. function to convert from tibble to pandas data frame\n\r# Then construct pandas data frame from tibble\rkgr_dt_heatmap = r.kgr_dt_df \rThen prepare the file to three dimension that the function sns.heatmap recognize. We arrange in such way that the index which represent Y-column become months, the column which represent the x-axis become year and the values to be filled are chlorophyll. The chunk below hihglights\n\rkgrpy_piv = pd.pivot_table(data=kgr_dt_heatmap, values=\u0026quot;chl\u0026quot;, index=\u0026#39;month\u0026#39;, columns=\u0026#39;year\u0026#39;)\rOnce we have prepared the dataset in the format that heatmap function from seaboarn understand we can plot. Figure 3 was generated using the chunk below;\n\rfig, axes = plt.subplots(nrows=1, ncols=1)\rsns.heatmap(data=kgrpy_piv, cmap = \u0026quot;jet\u0026quot;, xticklabels=True, yticklabels=True)\r# plt.gca().invert_yaxis()\r# plt.xlabel([39.4,39.6,39.8,40.0])\rplt.show()\r\rFigure 3: heatmaps showing variation of chlorophyll over years and months\r\rSimilarly, the heatmap of sea surface temperature shown in figure 4 was generated using the chunk below;\n\rkgrpy_piv_sst = pd.pivot_table(data=kgr_dt_heatmap, values=\u0026quot;sst\u0026quot;, index=\u0026#39;month\u0026#39;, columns=\u0026#39;year\u0026#39;)\rfig, axes = plt.subplots(nrows=1, ncols=1)\rsns.heatmap(data=kgrpy_piv_sst, cmap = \u0026quot;jet\u0026quot;, xticklabels=True, yticklabels=True)\r# plt.gca().invert_yaxis()\r# plt.xlabel([39.4,39.6,39.8,40.0])\rplt.show()\r\rFigure 4: heatmaps showing variation of SST over years and months\r\r\rPandas Dataframe to Tibble\rIn this section we use function from Python to read the dataset from the local machine into R session. The function pd.read_excell() function is used. The outocome after reading the file is the Pandas Dataframe.\nkgpy = pd.read_excel((\u0026quot;c:/Users/Semba/Documents/kuguru.xlsx\u0026quot;))\rOnce the Pandas Dataframe file is in the environment, we need to convert it to tibble data dataframe. A py function from reticulate package is used as the chunk below illustrates:\n\rnote that conversion from pandas to tibble data frame must be done in the R chunk and not Python chunk\n\rkg_r = py$kgpy\rWe then use the mutate function from dplyr to create new column called month and then select sst, chl, and month variables.\nkg_r = kg_r%\u0026gt;% mutate(month = lubridate::month(time, abbr = T, label = T))%\u0026gt;% select(sst,chl, month) \rThen, we can visualize the dataset with ggplot package. The chunk below was used to generate figure 5\nggplot(data = kg_r, aes(x = chl, y = sst, col = month))+\rgeom_point(size = 2)+\rlabs(x = expression(Chlorophyll~concentration(mgm^{-3})),\ry = expression(Sea~surface~temperature(degree*C)))+\rtheme(legend.key = element_blank())\r\rFigure 5: Relationship between chlorophyll concentration and sea surface temperature\r\r\r","permalink":"/post/combine-the-power-of-r-and-python-in-d/","tags":["Pandas","matplotlib","Masumbuko Semba","dplyr","ggplot2"],"title":"Integrate `R` and `Python` in Rstudio to Power Your Analytical Capability"},{"categories":[],"contents":"\rTidyverse is changing the way we do programming in R world. This ecosystem contains a collection of packages that ensure consistent data manipulation and visualisation space across the specturm of data science, which has modernized the R programming. The value of these tools has been so great that many of them have been ported to Python. That’s why we thought we should provide an introduction to tidyverse for Python blog post. One of the package of tidyverse is dplyr—a popular package for data manipulation in R programming. Because of its power,Python has developed a similarr to dplyr, called dplython. . You can simply install dplython in Python using the pip function\npip install dplython\rrequire(reticulate)\r## Loading required package: reticulate\ruse_python(\u0026quot;c:/Python/Anaconda3/\u0026quot;)\rWe first load the packages into the sessions\nimport dplython\rimport pandas as pd\rimport matplotlib.pyplot as plt\r# from ggplot import *\rLet’s load a few function from from the dplython module that we are going to use inside python\n\rfrom dplython import (DplyFrame,diamonds, select, sample_n, sample_frac, mutate, sift, summarize, group_by, arrange, DelayFunction, X, head, transmute)\rFor the examples in this post, I use a dataset that chlorophyll and sea surface temperature over fifty months. The file was organized in Excel spreadsheet. Let’s import a this file with the read_excel() function from Pandas module. The pd.read_excel() will automatically convert excel spreadsheet to Panda’s DataFrame\nbongo = pd.read_excel(\u0026quot;c:/Users/Semba/Documents/kuguru.xlsx\u0026quot;)\r## c:\\Python\\Anaconda3\\lib\\site-packages\\xlrd\\xlsx.py:266: PendingDeprecationWarning: This method will be removed in future versions. Use \u0026#39;tree.iter()\u0026#39; or \u0026#39;list(tree.iter())\u0026#39; instead.\r## for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\rbongo.head()\r## chl sst time month year\r## 0 0.264 25.380 2002-07-01 7 2002\r## 1 0.288 25.100 2002-08-01 8 2002\r## 2 0.306 25.352 2002-09-01 9 2002\r## 3 0.231 26.129 2002-10-01 10 2002\r## 4 0.152 27.680 2002-11-01 11 2002\rAfter we’ve read in our data, we need to convert it to an object called a DplyFrame, which we do using a method from dplython.\nbongo_dp = DplyFrame(bongo)\rbongo_dp.head()\r## chl sst time month year\r## 0 0.264 25.380 2002-07-01 7 2002\r## 1 0.288 25.100 2002-08-01 8 2002\r## 2 0.306 25.352 2002-09-01 9 2002\r## 3 0.231 26.129 2002-10-01 10 2002\r## 4 0.152 27.680 2002-11-01 11 2002\rSelect variables–columns\rOnce we have converted the file into DplyFrame object, we can perform “dplyr-like” operations in python. With dplython, selecting columns—variable of interest is done like below:\nbongo_dp \u0026gt;\u0026gt; select(X.time, X.chl, X.sst)\r## time chl sst\r## 0 2002-07-01 0.264 25.380\r## 1 2002-08-01 0.288 25.100\r## 2 2002-09-01 0.306 25.352\r## 3 2002-10-01 0.231 26.129\r## 4 2002-11-01 0.152 27.680\r## .. ... ... ...\r## 145 2014-08-01 0.217 25.398\r## 146 2014-09-01 0.202 25.608\r## 147 2014-10-01 0.157 26.395\r## 148 2014-11-01 0.108 28.037\r## 149 2014-12-01 0.087 29.215\r## ## [150 rows x 3 columns]\rYou notice how X is appended to each field name, which is a little different from the select command in dplyr. We also use\u0026gt;\u0026gt;instead of %\u0026gt;% as a pipe operator. If you’re not as familiar with dplyr from R, the pipe operator basically allows to you apply a function to the output of a function or object to the left the operator (the \u0026gt;\u0026gt; in dplython). In the case above, the select function is just being applied to state_info.\nTo select only the top 10 or bottom 10 rows, we can chain the head or tail methods, respectively, to our line of code above. Here the head and tail functions are being applied to the result to the left of each respective pipe operator.\n\rFilter records—rows\rUnlike dplyr that use filter to select records in rows, dplython use sift function. For example, if we want to get only the records where a sea surface temperature is above 29.4oC, we would write a code as:\nbongo_dp \u0026gt;\u0026gt; sift(X.sst \u0026gt; 29.4)\r## chl sst time month year\r## 32 0.146 29.466 2005-03-01 3 2005\r## 33 0.157 29.532 2005-04-01 4 2005\r## 80 0.124 29.430 2009-03-01 3 2009\r## 92 0.148 29.984 2010-03-01 3 2010\r## 93 0.123 29.631 2010-04-01 4 2010\r## 128 0.128 29.473 2013-03-01 3 2013\r\rSorting and arranging records\rTo sort our DplyFrame by a column, we can use the arrange function similar to dplyr:\nbongo_dp \u0026gt;\u0026gt; arrange(X.sst)\r## chl sst time month year\r## 25 0.311 24.944 2004-08-01 8 2004\r## 13 0.305 24.987 2003-08-01 8 2003\r## 73 0.186 24.990 2008-08-01 8 2008\r## 24 0.304 25.051 2004-07-01 7 2004\r## 37 0.239 25.059 2005-08-01 8 2005\r## .. ... ... ... ... ...\r## 32 0.146 29.466 2005-03-01 3 2005\r## 128 0.128 29.473 2013-03-01 3 2013\r## 33 0.157 29.532 2005-04-01 4 2005\r## 93 0.123 29.631 2010-04-01 4 2010\r## 92 0.148 29.984 2010-03-01 3 2010\r## ## [150 rows x 5 columns]\rWe may sort more than onve variable, for instance in the chunk below we sort first by sst and then by chl value in ascending order\nbongo_dp \u0026gt;\u0026gt; arrange(X.sst, X.chl)\r## chl sst time month year\r## 25 0.311 24.944 2004-08-01 8 2004\r## 13 0.305 24.987 2003-08-01 8 2003\r## 73 0.186 24.990 2008-08-01 8 2008\r## 24 0.304 25.051 2004-07-01 7 2004\r## 37 0.239 25.059 2005-08-01 8 2005\r## .. ... ... ... ... ...\r## 32 0.146 29.466 2005-03-01 3 2005\r## 128 0.128 29.473 2013-03-01 3 2013\r## 33 0.157 29.532 2005-04-01 4 2005\r## 93 0.123 29.631 2010-04-01 4 2010\r## 92 0.148 29.984 2010-03-01 3 2010\r## ## [150 rows x 5 columns]\r\rCreate New Variable\rLike dplyr, mutate function is used to compute and create new variable in the dataframe. For instance, we might be interested to\nbongo_dp \u0026gt;\u0026gt; mutate(ratio = X.sst / X.chl)\r## chl sst time month year ratio\r## 0 0.264 25.380 2002-07-01 7 2002 96.136364\r## 1 0.288 25.100 2002-08-01 8 2002 87.152778\r## 2 0.306 25.352 2002-09-01 9 2002 82.849673\r## 3 0.231 26.129 2002-10-01 10 2002 113.112554\r## 4 0.152 27.680 2002-11-01 11 2002 182.105263\r## .. ... ... ... ... ... ...\r## 145 0.217 25.398 2014-08-01 8 2014 117.041475\r## 146 0.202 25.608 2014-09-01 9 2014 126.772277\r## 147 0.157 26.395 2014-10-01 10 2014 168.121019\r## 148 0.108 28.037 2014-11-01 11 2014 259.601852\r## 149 0.087 29.215 2014-12-01 12 2014 335.804598\r## ## [150 rows x 6 columns]\r\rSummarizing data\rWe can aggregate data by chaining the summarize and group_by functions:\nbongo_dp \u0026gt;\u0026gt; group_by(X.month) \u0026gt;\u0026gt; summarize(chl_mean = X.chl.mean(), sst_mean = X.sst.mean())\r## month chl_mean sst_mean\r## 0 1 0.124000 28.617250\r## 1 2 0.132333 28.682083\r## 2 3 0.137417 29.241000\r## 3 4 0.139500 29.131250\r## 4 5 0.139583 27.987917\r## 5 6 0.166167 26.778083\r## 6 7 0.216538 25.635077\r## 7 8 0.227615 25.243385\r## 8 9 0.231846 25.463308\r## 9 10 0.180923 26.229308\r## 10 11 0.121538 27.571462\r## 11 12 0.100923 28.740615\rwastani = bongo_dp \u0026gt;\u0026gt; group_by(X.month) \u0026gt;\u0026gt; summarize(chl_mean = X.chl.mean(), sst_mean = X.sst.mean())\rimport seaborn as sns\r## c:\\Python\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\r## return f(*args, **kwds)\r## c:\\Python\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\r## return f(*args, **kwds)\r## c:\\Python\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\r## return f(*args, **kwds)\rfig = plt.figure()\rsns.pointplot(x=wastani.chl_mean, y=wastani.sst_mean, hue=wastani.sst_mean)\rplt.show()\r\rGet the frequency of observations\rTo get the count of the values in a specific column, use the count function. For example, we get number of observations of each year\n\rfrom dplython import count\rbongo_dp \u0026gt;\u0026gt; count(X.year)\r## year n\r## 0 2002 6.0\r## 1 2003 12.0\r## 2 2004 12.0\r## 3 2005 12.0\r## 4 2006 12.0\r## 5 2007 12.0\r## 6 2008 12.0\r## 7 2009 12.0\r## 8 2010 12.0\r## 9 2011 12.0\r## 10 2012 12.0\r## 11 2013 12.0\r## 12 2014 12.0\r\rSubsampling\rThe sample_n function can be used to randomly sample rows from our dataset. You just need to pass whatever number of rows you want to randomly select. For instance, in the chunk below, I want five random sample of observations from the dataset\n\rbongo_dp \u0026gt;\u0026gt; sample_n(5)\r## chl sst time month year\r## 33 0.157 29.532 2005-04-01 4 2005\r## 132 0.242 25.410 2013-07-01 7 2013\r## 93 0.123 29.631 2010-04-01 4 2010\r## 59 0.155 27.096 2007-06-01 6 2007\r## 133 0.248 25.110 2013-08-01 8 2013\rUnlike the sample_n, which select number of sample, sample_frac function allows to sample percatange of the data. For example, the chunk below highlight the randomly sample 5% of the obervations:\n\rbongo_dp \u0026gt;\u0026gt; sample_frac(.05)\r## chl sst time month year\r## 130 0.150 27.318 2013-05-01 5 2013\r## 72 0.173 25.420 2008-07-01 7 2008\r## 102 0.084 29.008 2011-01-01 1 2011\r## 47 0.163 26.630 2006-06-01 6 2006\r## 9 0.163 29.346 2003-04-01 4 2003\r## 71 0.150 26.814 2008-06-01 6 2008\r## 39 0.212 25.847 2005-10-01 10 2005\r## 118 0.136 27.283 2012-05-01 5 2012\r\r","permalink":"/post/dplython-dplyr-function-in-python/","tags":["Python","dplyr","dplython","Masumbuko Semba","data science","Pandas","R","R Markdown"],"title":"dplython: dplyr function in python"},{"categories":[],"contents":"\rPanda DataFrames\rPandas is an open source Python library that provide easy–to–use data structures organized in tabular format. Pandas is particularly designed to work with tabular data—data that has rows representing records and columns representing variables. The usage of Pandas in data science has grown exponentially in recent years because it allows scientist to work more efficient and effectively. In this post, I will demonstrate the basic capabilities of Pandas module. It is a deep libary, and I will touch the surface. For detailed information, I encourage you to consult the [Pandas’s] document(http://pandas.pydata.org/pandas-docs/stable/).\nrequire(reticulate)\r## Loading required package: reticulate\ruse_python(\u0026quot;c:/Python/Anaconda3/\u0026quot;)\rWe first load the packages into the sessions\nimport pandas as pd\rimport numpy as np\rfrom matplotlib import pyplot as plt\rPandas Data Structures: Series\rA series represent a one–deimentional array of data. The main difference between a series and numpy array is tha a series has an index. The index contains the labels that we use to access the data. There are various ways to creas a Pandas’s Series, in this post I will show you some of them\nsites = [\u0026#39;Pemba\u0026#39;, \u0026#39;Zanzibar\u0026#39;, \u0026#39;Mafia\u0026#39;]\rsst = [25.4,26.3,26.1]\rsst_df = pd.Series(sst, index=sites)\rsst_df\r## Pemba 25.4\r## Zanzibar 26.3\r## Mafia 26.1\r## dtype: float64\rPanda’s seris has built in plotting function\nsst_df.plot(kind = \u0026quot;bar\u0026quot;)\rplt.show()\rArithmetic operations and most numpy function can be applied to Series. An important point is that the Series keep their index during such operations.\nnp.log(sst_df/ sst_df**2)\r## Pemba -3.234749\r## Zanzibar -3.269569\r## Mafia -3.261935\r## dtype: float64\rWe can access the element in the series with index\nsst_df.index\r## Index([\u0026#39;Pemba\u0026#39;, \u0026#39;Zanzibar\u0026#39;, \u0026#39;Mafia\u0026#39;], dtype=\u0026#39;object\u0026#39;)\rWe can also use the .loc function to obtain the values of the element.Note the use of [] instead of ()\n\rsst_df.loc[\u0026quot;Pemba\u0026quot;]\r## 25.4\rWe can also access the value using the location\nsst_df.iloc[1]\r## 26.3\rWe can pass a list or array to loc to pick multiple rows\nsst_df[[\u0026quot;Pemba\u0026quot;, \u0026quot;Mafia\u0026quot;]]\r## Pemba 25.4\r## Mafia 26.1\r## dtype: float64\rAnd even use a slice notation\nsst_df[\u0026quot;Pemba\u0026quot;: \u0026quot;Mafia\u0026quot;]\r## Pemba 25.4\r## Zanzibar 26.3\r## Mafia 26.1\r## dtype: float64\rSimilar to index function that brings index names, We can obtain the values of the column using the values function\nsst_df.values\r## array([25.4, 26.3, 26.1])\r\rPandas Data Structures: DataFrame\rAlthous Pandas Series provide necessary tools and function for data manipulation, but it is limited to a single column. A more useful Pandas data structure is the DataFrame. A DataFrame bind together series that share the same index. Its like a table in a spreadsheet. First we construct a dictionary\ndata = {\u0026quot;sst\u0026quot;: [26.3,np.nan,25.3,25.8], \u0026quot;chl\u0026quot;: [0.12,0.21,0.32,np.nan], \u0026quot;rain\u0026quot;: [320,125,250,np.nan]}\rlet’s import a comma–separated data of Iris dataset with the read_excel() function from Pandas module. The pd.read_excel() will automatically convert excel spreadsheet to Panda’s DataFrame\ndf = pd.read_excel(\u0026quot;c:/Users/Semba/Documents/kuguru.xlsx\u0026quot;)\rdf\r## chl sst time month year\r## 0 0.264 25.380 2002-07-01 7 2002\r## 1 0.288 25.100 2002-08-01 8 2002\r## 2 0.306 25.352 2002-09-01 9 2002\r## 3 0.231 26.129 2002-10-01 10 2002\r## 4 0.152 27.680 2002-11-01 11 2002\r## .. ... ... ... ... ...\r## 145 0.217 25.398 2014-08-01 8 2014\r## 146 0.202 25.608 2014-09-01 9 2014\r## 147 0.157 26.395 2014-10-01 10 2014\r## 148 0.108 28.037 2014-11-01 11 2014\r## 149 0.087 29.215 2014-12-01 12 2014\r## ## [150 rows x 5 columns]\rA wide Pandas tools and functions are available to explore and analyse the data structured in Pandas DataFrame.\ndf.head(); df.tail()\r## chl sst time month year\r## 0 0.264 25.380 2002-07-01 7 2002\r## 1 0.288 25.100 2002-08-01 8 2002\r## 2 0.306 25.352 2002-09-01 9 2002\r## 3 0.231 26.129 2002-10-01 10 2002\r## 4 0.152 27.680 2002-11-01 11 2002\r## chl sst time month year\r## 145 0.217 25.398 2014-08-01 8 2014\r## 146 0.202 25.608 2014-09-01 9 2014\r## 147 0.157 26.395 2014-10-01 10 2014\r## 148 0.108 28.037 2014-11-01 11 2014\r## 149 0.087 29.215 2014-12-01 12 2014\rdf.min()\r## chl 0.084\r## sst 24.944\r## time 2002-07-01 00:00:00\r## month 1\r## year 2002\r## dtype: object\rdf.median()\r## chl 0.1435\r## sst 27.7165\r## month 7.0000\r## year 2008.0000\r## dtype: float64\rdf.max()\r## chl 0.339\r## sst 29.984\r## time 2014-12-01 00:00:00\r## month 12\r## year 2014\r## dtype: object\rdf.mean()\r## chl 0.160667\r## sst 27.404880\r## month 6.620000\r## year 2008.240000\r## dtype: float64\rdf.std()\r## chl 0.055021\r## sst 1.486930\r## month 3.461524\r## year 3.626302\r## dtype: float64\rdf.describe()\r## chl sst month year\r## count 150.000000 150.00000 150.000000 150.000000\r## mean 0.160667 27.40488 6.620000 2008.240000\r## std 0.055021 1.48693 3.461524 3.626302\r## min 0.084000 24.94400 1.000000 2002.000000\r## 25% 0.122250 25.83825 4.000000 2005.000000\r## 50% 0.143500 27.71650 7.000000 2008.000000\r## 75% 0.193000 28.78525 10.000000 2011.000000\r## max 0.339000 29.98400 12.000000 2014.000000\rWe can get a single column as a Series using python’s getitem syntax on the DataFrame object.\ndf[[\u0026#39;time\u0026#39;, \u0026#39;chl\u0026#39;]]\r## time chl\r## 0 2002-07-01 0.264\r## 1 2002-08-01 0.288\r## 2 2002-09-01 0.306\r## 3 2002-10-01 0.231\r## 4 2002-11-01 0.152\r## .. ... ...\r## 145 2014-08-01 0.217\r## 146 2014-09-01 0.202\r## 147 2014-10-01 0.157\r## 148 2014-11-01 0.108\r## 149 2014-12-01 0.087\r## ## [150 rows x 2 columns]\rWe can also index using a boolean series. This is very useful\ndf[df.sst \u0026gt; 29.4]\r## chl sst time month year\r## 32 0.146 29.466 2005-03-01 3 2005\r## 33 0.157 29.532 2005-04-01 4 2005\r## 80 0.124 29.430 2009-03-01 3 2009\r## 92 0.148 29.984 2010-03-01 3 2010\r## 93 0.123 29.631 2010-04-01 4 2010\r## 128 0.128 29.473 2013-03-01 3 2013\rPlotting\n\rimport seaborn as sns\rplt.figure(figsize=(6, 4))\rsns.pairplot(data=df)\r## \u0026lt;seaborn.axisgrid.PairGrid object at 0x0000000027E7F808\u0026gt;\rplt.show()\r\r","permalink":"/post/pandas-data-structures/","tags":["Pandas","Python"],"title":"Pandas Data Structures"},{"categories":[],"contents":"\rA recent findings proved the hard way we human h…\n","permalink":"/projects/the-way-we-make-life/","tags":[],"title":"The way we make life"},{"categories":[],"contents":"\rToday we will learn about Cartopy, one of the most common packages for making maps within python. Another popular and powerful library is Basemap; however, Basemap is going away and being replaced with Cartopy in the near future, For this reason, investing your time in learning mapping in python with Cartopy module is recommended.\nWe thank Research in Computing Earth Sciences because most of material in this post are gleaned from their website. I also thank Phil Elson, a developer of Cartopy and created excellent Cartopy Tutorial\nCartopy makes use of the powerful PROJ.4, numpy and shapely libraries and includes a programatic interface built on top of Matplotlib for the creation of publication quality maps. Key features of cartopy are its object oriented projection definitions, and its ability to transform points, lines, vectors, polygons and images between those projections.\nCartopy Projections and other reference systems\rIn Cartopy, each projection is a class. Most classes of projection can be configured in projection-specific ways, although Cartopy takes an opinionated stance on sensible defaults. Let’s create a Plate Carree projection instance.To do so, we need cartopy’s crs module. This is typically imported as ccrs (Cartopy Coordinate Reference Systems).\nBefore we import the modules we need for mapping, we first have to initialize the linkage of python in Rstudio using the reticulate package and also set the environment in which the seesion will fetch python functions and package. You must load the reticulate package and set the enviroment while in R chunk;\nrequire(reticulate)\r## Loading required package: reticulate\ruse_python(\u0026quot;c:/Python/Anaconda3/\u0026quot;)\rWe then import some Python’s modules using the import function. Make sure that you insert the Pyhon chunk for you to load these modules\n\rimport numpy as np\rimport pandas as pd\rimport cartopy.crs as ccrs\rimport cartopy\rimport matplotlib.pyplot as plt\rWe can access the Mollweide projection with the code chunk below;\nccrs.Mollweide()\r## \u0026lt;cartopy.crs.Mollweide object at 0x0000000023B9CD68\u0026gt;\r\rDrawing a map\rCartopy optionally depends upon matplotlib, and each projection knows how to create a matplotlib Axes (or AxesSubplot) that can represent itself.\nThe Axes that the projection creates is a cartopy.mpl.geoaxes.GeoAxes. This Axes subclass overrides some of matplotlib’s existing methods, and adds a number of extremely useful ones for drawing maps.\nWe’ll go back and look at those methods shortly, but first, let’s actually see the cartopy+matplotlib dance in action:\nimport matplotlib.pyplot as plt\rplt.axes(projection=ccrs.PlateCarree())\rplt.show()\rThat was a little underwhelming, but we can see that the Axes created is indeed one of those GeoAxes[Subplot] instances.\nOne of the most useful methods that this class adds on top of the standard matplotlib Axes class is the coastlines method. With no arguments, it will add the Natural Earth 1:110,000,000 scale coastline data to the map.\n\rplt.figure()\rax = plt.axes(projection=ccrs.PlateCarree())\rax.coastlines()\rplt.show()\rWe could just as equally created a matplotlib subplot with one of the many approaches that exist. For example, the plt.subplots function could be used:\n\rfig, ax = plt.subplots(subplot_kw={\u0026#39;projection\u0026#39;: ccrs.PlateCarree()})\rax.coastlines()\rplt.show\r## \u0026lt;function make_python_function.\u0026lt;locals\u0026gt;.python_function at 0x0000000023A758B8\u0026gt;\rProjection classes have options we can use to customize the map so that Africa is at the center\nax = plt.axes(projection=ccrs.PlateCarree(central_longitude=0))\rax.coastlines()\rplt.show()\r\rUseful methods of a GeoAxes\rThe cartopy.mpl.geoaxes.GeoAxes class adds a number of useful methods. Let’s take a look at:\nset_global - zoom the map out as much as possible\rset_extent - zoom the map to the given bounding box\rgridlines - add a graticule (and optionally labels) to the axes\rcoastlines - add Natural Earth coastlines to the axes\rstock_img - add a low-resolution Natural Earth background image to the axes\rimshow - add an image (numpy array) to the axes\radd_geometries - add a collection of geometries (Shapely) to the axes\r\rSome More Examples of Different Global Projections\n\rprojections = [ccrs.PlateCarree(),\rccrs.Robinson(),\rccrs.Mercator(),\rccrs.Orthographic(),\rccrs.InterruptedGoodeHomolosine()\r]\rfor proj in projections:\rplt.figure()\rax = plt.axes(projection=proj)\rax.stock_img()\rax.coastlines()\rax.set_title(f\u0026#39;{type(proj)}\u0026#39;)\rplt.show()\r\rRegional Maps\rTo create a regional map, we use the set_extent method of GeoAxis to limit the size of the region.\ncentral_lon =43, central_lat = -8.5\rextent = [35, 50, -16.5, 0]\rax = plt.axes(projection=ccrs.PlateCarree(central_lon, central_lat))\rax.set_extent(extent)\rax.gridlines()\rax.coastlines(resolution=\u0026#39;50m\u0026#39;)\rplt.show()\r\rAdding Features to the Map\rTo give our map more styles and details, we add cartopy.feature objects. Many useful features are built in. These “default features” are at coarse (110m) resolution.\ncartopy.feature.BORDERS Country boundaries\rcartopy.feature.COASTLINE Coastline, including major islands\rcartopy.feature.LAKES Natural and artificial lakes\rcartopy.feature.LAND Land polygons, including major islands\rcartopy.feature.OCEAN Ocean polygons\rcartopy.feature.RIVERS Single-line drainages, including lake centerlines\rcartopy.feature.STATES (limited to the United States at this scale)\r\rimport cartopy.feature as cfeature\rimport numpy as np\rcentral_lat = 37.5\rcentral_lon = -96\rextent = [28, 45, -25, 2]\rcentral_lon = np.mean(extent[:2])\rcentral_lat = np.mean(extent[2:])\rplt.figure(figsize=(6, 6))\rax = plt.axes(projection=ccrs.EquidistantConic(central_lon, central_lat))\rax.set_extent(extent)\rax.add_feature(cartopy.feature.OCEAN)\rax.add_feature(cartopy.feature.LAND, edgecolor=\u0026#39;black\u0026#39;)\rax.add_feature(cartopy.feature.LAKES, edgecolor=\u0026#39;black\u0026#39;)\rax.add_feature(cartopy.feature.RIVERS)\rax.gridlines()\r# ax.gridlines(draw_labels=True, xlocs=[32, 36, 40, 44])\r## \u0026lt;cartopy.mpl.gridliner.Gridliner object at 0x0000000027FA9D48\u0026gt;\rplt.show()\rrivers_50m = cfeature.NaturalEarthFeature(\u0026#39;physical\u0026#39;, \u0026#39;rivers_lake_centerlines\u0026#39;, \u0026#39;10m\u0026#39;)\rcentral_lat = 37.5\rcentral_lon = -96\rextent = [28, 45, -25, 2]\rcentral_lon = np.mean(extent[:2])\rcentral_lat = np.mean(extent[2:])\rplt.figure(figsize=(6, 6))\rax = plt.axes(projection=ccrs.EquidistantConic(central_lon, central_lat))\rax.set_extent(extent)\rax.add_feature(cartopy.feature.OCEAN)\rax.add_feature(cartopy.feature.LAND, edgecolor=\u0026#39;black\u0026#39;)\rax.add_feature(cartopy.feature.LAKES, edgecolor=\u0026#39;black\u0026#39;)\rax.add_feature(rivers_50m, facecolor=\u0026#39;None\u0026#39;, edgecolor=\u0026#39;b\u0026#39;)\rax.gridlines()\r## \u0026lt;cartopy.mpl.gridliner.Gridliner object at 0x0000000027C3A588\u0026gt;\rplt.show()\r\rPlotting 2D (Raster) Data\rThe same principles apply to 2D data. Below we create some example data defined in regular lat / lon coordinates. for this case we will load the global sea surface temperature data. The data is stored as netcdf format and hence we need to load the\n\rimport netCDF4 as nc\rThen we a Dataset function from the netCDF4 module to read the file\nsst = nc.Dataset(\u0026quot;e:/MatlabWorking/GHRSST/20150101.nc\u0026quot;)\rWe need to extract different variables that are stored in the file. But before we extract them, we must look on the internal structure of the file and identify the variables with correct names. We can do that using the nc.variables function\nsst.variables\rWe noticed that the file is the array of lon. lat, analysed_sst, and time. The time is the single interval. Then, we extract the variables as the chunk below shows;\ntime = sst.variables[\u0026#39;time\u0026#39;]\rlon = sst.variables[\u0026#39;lon\u0026#39;]\rlat = sst.variables[\u0026#39;lat\u0026#39;]\rdata = sst.variables[\u0026#39;analysed_sst\u0026#39;]\rBecause the data is in the rectangular grid, we also need to convert the lon and lat to rectangular grid with np.meshgrid(). The purpose of meshgrid is to create a rectangular grid out of an array of x values and an array of y values\n\rlon2d, lat2d = np.meshgrid(lon, lat)\rBecause the temperature was recorded in Kelvin scale, we can simpy convert to degree by simply substracting with 273\n\rdatar = data[0,:,:]-273\r# datar = np.flipud(datar)\rThen we can map the spatial distribution of sea surface temperature around the global as shown in figure 1\n\rplt.figure(figsize=(6,5))\rax = plt.axes(projection=ccrs.PlateCarree())\rax.set_global()\r# ax.set_extent([-170,170,-30,30])\rax.coastlines()\rax.contourf(lon2d, lat2d, datar, cmap = \u0026quot;jet\u0026quot;)\r## \u0026lt;matplotlib.contour.QuadContourSet object at 0x0000000027FA0C88\u0026gt;\rax.gridlines(draw_labels=True, xlocs=np.arange(-180,180,30))\r## \u0026lt;cartopy.mpl.gridliner.Gridliner object at 0x0000000039A11908\u0026gt;\rplt.show()\r\rFigure 1: Sea surface temperature\r\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport cartopy.crs as ccrs\rimport cartopy.feature as cfeature\rfrom matplotlib.offsetbox import AnchoredText\rfig = plt.figure()\r# ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\rax = plt.axes(projection=ccrs.PlateCarree())\rax.set_extent([80, 170, -45, 30], crs=ccrs.PlateCarree())\r# Put a background image on for nice sea rendering.\rax.stock_img()\r# Create a feature for States/Admin 1 regions at 1:50m from Natural Earth\rstates_provinces = cfeature.NaturalEarthFeature(\rcategory=\u0026#39;cultural\u0026#39;,\rname=\u0026#39;admin_1_states_provinces_lines\u0026#39;,\rscale=\u0026#39;50m\u0026#39;,\rfacecolor=\u0026#39;none\u0026#39;)\rSOURCE = \u0026#39;Natural Earth\u0026#39;\rLICENSE = \u0026#39;public domain\u0026#39;\rax.add_feature(cfeature.LAND)\rax.add_feature(cfeature.COASTLINE)\rax.add_feature(states_provinces, edgecolor=\u0026#39;gray\u0026#39;)\r# Add a text annotation for the license information to the\r# the bottom right corner.\rtext = AnchoredText(r\u0026#39;$\\mathcircled{{c}}$ {}; license: {}\u0026#39;\r\u0026#39;\u0026#39;.format(SOURCE, LICENSE),\rloc=4, prop={\u0026#39;size\u0026#39;: 12}, frameon=True)\rax.add_artist(text)\rplt.show()\r\r\rfig = plt.figure()\rax = plt.axes(projection=ccrs.PlateCarree())\rax.set_extent([20, 70, -40, 20], crs=ccrs.PlateCarree())\rax.add_feature(cfeature.LAND, edgecolor=\u0026#39;gray\u0026#39;)\rax.add_feature(cfeature.BORDERS)\rax.add_feature(cfeature.COASTLINE)\rax.gridlines(xlocs=np.arange(20,70,10), draw_labels=True, crs=ccrs.PlateCarree())\r## \u0026lt;cartopy.mpl.gridliner.Gridliner object at 0x000000003EF45148\u0026gt;\rplt.show()\r\rfig = plt.figure()\rax = plt.axes(projection=ccrs.PlateCarree())\rax.set_extent([20, 70, -40, 20], crs=ccrs.PlateCarree())\r# Put a background image on for nice sea rendering.\rax.stock_img()\rax.add_feature(cfeature.BORDERS)\rax.add_feature(cfeature.COASTLINE)\rax.gridlines(xlocs=np.arange(20,70,10), draw_labels=True, crs=ccrs.PlateCarree())\r## \u0026lt;cartopy.mpl.gridliner.Gridliner object at 0x000000003F4AF148\u0026gt;\rplt.show()\rfig = plt.figure()\rax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\rax.set_extent([-20, 60, -40, 45], crs=ccrs.PlateCarree())\rax.stock_img()\rax.add_feature(cfeature.LAND)\rax.add_feature(cfeature.OCEAN)\rax.add_feature(cfeature.COASTLINE)\rax.add_feature(cfeature.BORDERS, linestyle=\u0026#39;dotted\u0026#39;)\rax.add_feature(cfeature.LAKES, alpha=0.5)\rax.add_feature(cfeature.RIVERS)\rplt.show()\r\rGlobal Map\rAn example of a simple map that compares Geodetic and Plate Carree lines between two locations.\n fig = plt.figure(figsize=(8, 4))\rax = fig.add_subplot(1, 1, 1, projection=ccrs.Robinson(central_longitude=0))\rax.set_global()\rax.stock_img()\rax.coastlines()\rax.gridlines()\r# ax.plot(-0.08, 51.53, \u0026#39;o\u0026#39;, transform=ccrs.PlateCarree())\r# ax.plot([-0.08, 132], [51.53, 43.17], transform=ccrs.PlateCarree())\r# ax.plot([-0.08, 132], [51.53, 43.17], transform=ccrs.Geodetic())\r## \u0026lt;cartopy.mpl.gridliner.Gridliner object at 0x000000003FA54488\u0026gt;\rplt.show()\r\rFilled contours\rWe use the gridded sea surface temperature and because I want to see the pacific and Indian ocean, I used the Mollweide projection and center the longitude to 130.\n## read the netcdf file with Dataset function from netCDF4 module\rsst = nc.Dataset(\u0026quot;e:/MatlabWorking/GHRSST/20050101.nc\u0026quot;)\r## sst.variables\r## extract individual varibales in the nc file\rtime = sst.variables[\u0026#39;time\u0026#39;][:]\rlon = sst.variables[\u0026#39;lon\u0026#39;][:]\rlat = sst.variables[\u0026#39;lat\u0026#39;][:]\rsst = sst.variables[\u0026#39;analysed_sst\u0026#39;]\r## extract an array of the first day and convert temperature from Kelvin to Degree Celsius\rsstr = sst[0,:,:]-273\r## create a rectangular of the long and lat\rlon2d, lat2d = np.meshgrid(lon, lat)\rplt.figure(figsize=(8,4))\rax = plt.axes(projection=ccrs.Mollweide(central_longitude=160),)\rax.set_global()\r# ax.set_extent([-170,170,-30,30])\rax.coastlines()\rax.contourf(lon2d, lat2d, datar, transform=ccrs.PlateCarree(), cmap=\u0026#39;nipy_spectral\u0026#39;)\r# ax.gridlines(draw_labels=True, xlocs=np.arange(-180,180,30))# unsupported with Mollweide\r## \u0026lt;matplotlib.contour.QuadContourSet object at 0x000000003FA54108\u0026gt;\rplt.show()\rWe can also read and map the mean sea level anomaly as the chunk below illustrate\n## read mean sea level anomaly data\rmsla = nc.Dataset(\u0026quot;e:/MatlabWorking/Altimetry/msla_h/indian_ocean-twosat-msla-h_010193_311295.nc\u0026quot;)\r## msla.variables\rtime = msla.variables[\u0026#39;time\u0026#39;][:]\rlon = msla.variables[\u0026#39;lon\u0026#39;][:]\rlat = msla.variables[\u0026#39;lat\u0026#39;][:]\rsla = msla.variables[\u0026#39;sla\u0026#39;]\rsla30 = sla[30,:,:]\rlon2d, lat2d = np.meshgrid(lon, lat)\rplt.figure(figsize=(6,5))\rax = plt.axes(projection=ccrs.Mollweide(central_longitude=60))\r# ax.set_global()\rax.set_extent([20,120,-50,20])\rax.coastlines()\r# ax.add_feature(cfeature.BORDERS, linestyle = \u0026quot;dotted\u0026quot;) # uncomment to plot country boundaries\rax.contourf(lon2d, lat2d, sla30, transform=ccrs.PlateCarree(), cmap=\u0026#39;nipy_spectral\u0026#39;)\r## \u0026lt;matplotlib.contour.QuadContourSet object at 0x000000003E46FD08\u0026gt;\rax.gridlines(draw_labels=False, xlocs=np.arange(-180,180,30))\r## \u0026lt;cartopy.mpl.gridliner.Gridliner object at 0x000000003A0D5688\u0026gt;\rplt.show()\rfrom sklearn import preprocessing\rsla30[sla30 == -2147483647] = np.nan\rnormalized_sla = preprocessing.normalize(sla30)\rsla30_nan = sla30.copy\rsla30_nan[sla30_nan == -2147483647] = np.nan\rplt.figure(figsize=(6,5))\rax = plt.axes(projection=ccrs.Mollweide(central_longitude=60))\r# ax.set_global()\rax.set_extent([20,120,-50,20])\rax.coastlines()\r# ax.add_feature(cfeature.BORDERS, linestyle = \u0026quot;dotted\u0026quot;) # uncomment to plot country boundaries\rax.contourf(lon2d, lat2d, sla30, transform=ccrs.PlateCarree(), cmap=\u0026#39;nipy_spectral\u0026#39;)\rax.gridlines(draw_labels=False, xlocs=np.arange(-180,180,30))\rplt.show()\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport iris\rimport iris.plot as iplt\rimport iris.quickplot as qplt\rimport iris.analysis.cartography\r\r","permalink":"/post/mapping-with-cartopy-in-python/","tags":["data science","mapping","Masumbuko Semba","cartopy"],"title":"Mapping with Cartopy in Python"},{"categories":[],"contents":"\rDuring the last couple of decades, Matlab has been the most commonly-used scripting language in physical oceanography, and it has a large user base in many other fields— however, Python has been gaining ground, often being adopted by former Matlab users as well as by newcomers. Here is a little background to help you understand this shift, and why we advocate using Python from the start.\nPython was designed by a computer scientist as a general–purpose scripting language for easy adoption and widespread use. People tried it and liked it, and the result is that it is widely used throughout the software world, for all sorts of tasks, large and small. There is a vast array of Python packages that are freely available to do all sorts of things—including the sorts of things that oceanographers and other scientists do; but these packages are not neatly bound up in a single product, and the documentation for the language itself and for the packages is similarly scattered and of varying quality.\nWhy use Python instead of Matlab?\rPython is fundamentally a better computer language in many ways.\nIt is suitable for a wider variety of tasks.\rIt scales better from the shortest of scripts to large software projects.\rIt facilitates writing clearer and more concise code.\rWith associated tools, it makes for easier access to existing high-performance codes in compiled languages, and for using smaller pieces of compiled code to speed up critical sections.\rBecause Python is Free and Open Source Software (FOSS), you can install it on any machine without having to deal with a license manager.\rFor the same reason, Python code that is part of a research project can be run by anyone, anywhere, to verify or extend the results.\rMost Python packages you are likely to want to use are developed in an open environment. The scientific Python ecosystem is dynamic and friendly.\r\rWhat are the potential disadvantages?\nInstallation of all the packages one needs can take time and expertise; but distributions like Anaconda, combined with other improvements in python packaging software and repositories, are rapidly solving this problem.\rAlthough progress is being made, the scientific Python stack is not as uniformly well-documented as Matlab; it might take longer to figure out how to do something in Python. You might also find that a routine available in Matlab is not yet available in a Python package.\rMatlab is still mainstream in oceanography–at least among many of the old guard; with Python, you are an early adopter. (If you have a spirit of adventure, this might be considered an advantage.)\r\rWhat about R?\rThe R language (and its commercial counterpart, S) is specialized for statistical calculations and plots. It is widely used in the social sciences but it is less suitable than Python for general computing in oceanography. If one has a need for some of its statistical capabilities, they can be accessed from Python using the interface module, Rpy2.\nrequire(reticulate)\r## Loading required package: reticulate\ruse_python(\u0026quot;c:/Python/Anaconda3/\u0026quot;)\rimport netCDF4 as nc\rimport matplotlib.pyplot as plt\rimport numpy as np\rimport pandas as pd\rimport seaborn as sns\r# from mpl_toolkits.basemap import Basemap\rsst = nc.Dataset(\u0026quot;e:/MatlabWorking/GHRSST/20150101.nc\u0026quot;)\rsst.variables\r## {\u0026#39;lat\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## float32 lat(lat)\r## long_name: latitude\r## standard_name: latitude\r## axis: Y\r## units: degrees_north\r## comment: uniform grid from -89.875 to 89.875 by 0.25\r## unlimited dimensions: ## current shape = (720,)\r## filling on, default _FillValue of 9.969209968386869e+36 used, \u0026#39;lon\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## float32 lon(lon)\r## long_name: longitude\r## standard_name: longitude\r## axis: X\r## units: degrees_east\r## comment: uniform grid from -179.875 to 179.875 by 0.25\r## unlimited dimensions: ## current shape = (1440,)\r## filling on, default _FillValue of 9.969209968386869e+36 used, \u0026#39;time\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int32 time(time)\r## long_name: reference time of sst field\r## standard_name: time\r## axis: T\r## units: seconds since 1981-01-01 00:00:00\r## unlimited dimensions: ## current shape = (1,)\r## filling on, default _FillValue of -2147483647 used, \u0026#39;analysed_sst\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int16 analysed_sst(time, lat, lon)\r## long_name: analysed sea surface temperature\r## standard_name: sea_surface_temperature\r## type: foundation\r## units: kelvin\r## _FillValue: -32768\r## add_offset: 273.15\r## scale_factor: 0.01\r## valid_min: -300\r## valid_max: 4500\r## unlimited dimensions: ## current shape = (1, 720, 1440)\r## filling on, \u0026#39;analysis_error\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int16 analysis_error(time, lat, lon)\r## long_name: estimated error standard deviation of analysed_sst\r## units: kelvin\r## _FillValue: -32768\r## add_offset: 0.0\r## scale_factor: 0.01\r## valid_min: 0\r## valid_max: 127\r## unlimited dimensions: ## current shape = (1, 720, 1440)\r## filling on, \u0026#39;mask\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int8 mask(time, lat, lon)\r## long_name: sea/land field composite mask\r## _FillValue: -128\r## flag_values: 1\r## flag_meanings: sea land lake ice\r## comment: b0:1=grid cell is open sea water b1:1=land is present in this grid cell b2:1=lake surface is present in this grid cell b3:1=sea ice is present in this grid cell b4-b7:reserve for future grid mask data\r## unlimited dimensions: ## current shape = (1, 720, 1440)\r## filling on, \u0026#39;sea_ice_fraction\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int8 sea_ice_fraction(time, lat, lon)\r## long_name: sea ice area fraction\r## standard_name: sea ice area fraction\r## units: percent\r## _FillValue: -128\r## add_offset: 0.0\r## scale_factor: 0.01\r## valid_min: 0\r## valid_max: 100\r## unlimited dimensions: ## current shape = (1, 720, 1440)\r## filling on}\rtime = sst.variables[\u0026#39;time\u0026#39;]\rlon = sst.variables[\u0026#39;lon\u0026#39;]\rlat = sst.variables[\u0026#39;lat\u0026#39;]\rdata = sst.variables[\u0026#39;analysed_sst\u0026#39;]\rdata.dimensions\r## (\u0026#39;time\u0026#39;, \u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;)\rfig, axes = plt.subplots(nrows=1, ncols=1)\rplt.imshow(data[0,:,:]-273,)\rplt.colorbar()\r## \u0026lt;matplotlib.colorbar.Colorbar object at 0x0000000027C68F88\u0026gt;\rplt.show()\rWe noticed that the data is flipped upside down. we need to correct and map it in the correct orientation. we use np.flipud function from numpy module for correcting the orientation\n\rdatar = np.flipud(data[0,:,:]-273)\rfig, axes = plt.subplots(nrows=1, ncols=1)\rplt.imshow(datar, cmap = \u0026quot;jet\u0026quot;)\rplt.colorbar()\r## \u0026lt;matplotlib.colorbar.Colorbar object at 0x000000002BF3ADC8\u0026gt;\rplt.clim(5,30)\rplt.show()\r# import dependencies\rimport os\ros.environ[\u0026#39;PROJ_LIB\u0026#39;] = \u0026#39;C:/Python/Anaconda3/Lib/site-packages/mpl_toolkits/basemap\u0026#39;\r# note that Basemap will be supported until 2020 only\rfrom mpl_toolkits.basemap import Basemap\rimport matplotlib.pyplot as plt\rimport numpy as np\r# make sure to set your plot size before you start rendering to screen\rplt.figure(figsize=(8, 8))\r# by default, only crude and low resolutions are installed, if you wanted\r# higher fidelity images, refer to the documentation.\rdefault_map = Basemap(projection=\u0026#39;ortho\u0026#39;, lat_0=45, lon_0=-105,\rresolution=\u0026#39;l\u0026#39;, area_thresh=1000.0)\rdefault_map.drawcoastlines()\rdefault_map.drawcountries() # matplotlib provides color creation default_map.fillcontinents(color=\u0026#39;navajowhite\u0026#39;)\rdefault_map.drawmapboundary()\rplt.show()\rread data from local file as pandas data frame and print the five records\n\r## scot = pd.read_csv(\u0026quot;e:/Data Manipulation/rpy/scottish_hills.csv\u0026quot;)\rscot.head()\r## Hill Name Height Latitude Longitude Osgrid\r## 0 A\u0026#39; Bhuidheanach Bheag 936.0 56.870342 -4.199001 NN660775\r## 1 A\u0026#39; Chailleach 997.0 57.693800 -5.128715 NH136714\r## 2 A\u0026#39; Chailleach 929.2 57.109564 -4.179285 NH681041\r## 3 A\u0026#39; Chraileag (A\u0026#39; Chralaig) 1120.0 57.184186 -5.154837 NH094147\r## 4 A\u0026#39; Ghlas-bheinn 918.0 57.255090 -5.303687 NH008231\rBecause of the error we experience in mapping with the basemap package, I will switch to an aternate module called cartopy. Unfortunately, Cartopy is not installed with Anaconda and you need to install it in your machine. The easiest way to install Cartopy on your own computer is with the package manager conda which you can access by typing into a terminal:\nconda install -c conda-forge cartopy\rThen you can load the modules in the session\n\rimport cartopy.crs as ccrs\rfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\rimport cartopy.feature as cfeature\rThen the chunk below\n\rplt.figure(figsize=(2.5,2.5))\rax = plt.axes(projection=ccrs.Mercator())\rax.coastlines(\u0026#39;10m\u0026#39;)\rax.xaxis.set_visible(True)\rax.yaxis.set_visible(True)\rax.set_yticks([56,57,58,59], crs=ccrs.PlateCarree())\r## [\u0026lt;matplotlib.axis.YTick object at 0x000000002D2A5B48\u0026gt;, \u0026lt;matplotlib.axis.YTick object at 0x000000002D2A0E08\u0026gt;, \u0026lt;matplotlib.axis.YTick object at 0x000000002D28C048\u0026gt;, \u0026lt;matplotlib.axis.YTick object at 0x000000002D2D6208\u0026gt;]\rax.set_xticks([-8, -6, -4, -2], crs=ccrs.PlateCarree())\r## [\u0026lt;matplotlib.axis.XTick object at 0x000000002D2A0288\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x000000002D28C648\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x000000002D28C148\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x000000002D2DC3C8\u0026gt;]\rlon_formatter = LongitudeFormatter(zero_direction_label=True)\rlat_formatter = LatitudeFormatter()\rax.xaxis.set_major_formatter(lon_formatter)\rax.yaxis.set_major_formatter(lat_formatter)\rax.set_extent([-8, -1.5, 55.3, 59])\rplt.scatter(scot[\u0026#39;Longitude\u0026#39;],scot[\u0026#39;Latitude\u0026#39;],\rcolor=\u0026#39;red\u0026#39;, marker=\u0026#39;^\u0026#39;, transform=ccrs.PlateCarree())\rplt.show()\r# plt.savefig(\u0026quot;munros.png\u0026quot;)\r\r","permalink":"/post/python-for-oceeanographers-and-marine-scientists/","tags":["Ocenography","marine","science","data science","Masumbuko Semba","Masumbuko","Semba"],"title":"Python for Oceeanographers and Marine Scientists"},{"categories":[],"contents":"\rdlab organized the second Data Tamasha from 4 to 6 December, 2019, that provide an opportunity to company, goverment organization, students and non-govermental organizationto showcase some of the ground–breaking solution in this digital age era. One of the key points from this Tamasha was the use of data science languages as tools in a toolkit. R, Python… Use them both. Leverage their strengths. Don’t build an “R Shop” or a “Python Shop”. Build a High Performance Data Science Team that capitalizes on the unique strengths of both languages.\nWhen you use multiple languages, you gain the ability to select the best tool for the job. The result is a language harmony that increases the data science team’s efficiency, capability, and productivity. The general idea is to be as flexible as possible so we can leverage the best of both languages within our full-stack data science workflow, which includes:\n\rEfficiently exploring data\rModeling, Cross Validating, and Evaluating Model Quality\rCommunicating data science to make better decisions via traditional reports (Word, PowerPoint, Excel, PDF), web-based reports (HTML), and interactive web-applications (Shiny, Django)\rWe can make a slight modification to the R and Python Strengths visualization to organize it in a logical sequence that leverages the strengths:\rR is selected for exploration because of the tidyverse readability and efficiency\rPython is selected for machine learning because of Scikit Learn machine learning pipeline capability\rR is selected for communication because of the advanced reporting utilities including RMarkdown and Shiny (interactive web apps) and the wonderful ggplot2 visualization package\r\rThe reticulate package includes a Python engine for R Markdown that enables easy interoperabilty between Python and R—the two widely used programming languages for data science. We first load the package in R chunk\nrequire(reticulate)\r## Loading required package: reticulate\rrequire(tidyverse)\r## Loading required package: tidyverse\r## -- Attaching packages ---------------------------------------------- tidyverse 1.3.0 --\r## v ggplot2 3.2.1 v purrr 0.3.3\r## v tibble 2.1.3 v dplyr 0.8.3\r## v tidyr 1.0.0 v stringr 1.4.0\r## v readr 1.3.1 v forcats 0.4.0\r## -- Conflicts ------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rBy default, reticulate package use the version of Python found on your PC PATH. However, if you have more than one version of Python installed in your machine, you can use the use_python function to use an alternative version.\nuse_python(\u0026quot;c:/Python/Anaconda3/\u0026quot;)\rFor use to call the Python libraries, we need to call them from the Python Chunk and not R chunk. Python chunk all execute within a single Python session to have access to all objects created in previous chunk. Like the R chunk, all chunk options works in similar manner in Python chunk. To access python function, we first have to import their corresponding libraries.\n\rimport pandas as pd\rWe use the read_excel() function from pandas library to import the file from the PC to R session\n\rsst = pd.read_excel(\u0026quot;E:/Data Manipulation/Temperature data/processing/Chumbe_SST_Temperature 1997-30Nov2017_IMS_Muhando.xlsx\u0026quot;)\rsst\r## day 1997 1998 ... 2015 2016 2017\r## 0 1997-01-01 27.830167 29.779000 ... 28.914583 28.942917 29.215833\r## 1 1997-01-02 27.889833 29.669333 ... 28.829583 28.886250 29.354375\r## 2 1997-01-03 27.932833 29.495333 ... 28.631250 28.928750 29.420833\r## 3 1997-01-04 28.025000 29.241333 ... 28.517917 28.893333 29.532083\r## 4 1997-01-05 28.058333 28.829000 ... 28.397500 28.935833 29.744792\r## .. ... ... ... ... ... ... ...\r## 361 1997-12-27 29.380139 29.329375 ... 28.850833 29.096458 NaN\r## 362 1997-12-28 29.379722 29.334792 ... 28.709167 28.665625 NaN\r## 363 1997-12-29 29.469583 29.330833 ... 28.872083 28.734792 NaN\r## 364 1997-12-30 29.616111 29.042222 ... 28.964167 28.966458 NaN\r## 365 1997-12-31 29.839583 29.086389 ... 28.900417 29.173750 NaN\r## ## [366 rows x 22 columns]\rimport seaborn as sns\rimport matplotlib.pyplot as plt\r# fig, axes = plt.subplots(nrows=1, ncols=1)\r# fig, axes = plt.subplot(nrows = 1, ncols = 1)\rsns.lineplot(x=\u0026#39;day\u0026#39;, y= \u0026#39;1997\u0026#39;, data=sst)\rplt.show()\rreticulate::py$sst %\u0026gt;% as_tibble()\r## # A tibble: 366 x 22\r## day `1997` `1998` `1999` `2000` `2001` `2002` `2003` `2004`\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1997-01-01 00:00:00 27.8 29.8 29.1 28.3 28.7 28.3 NaN 28.4\r## 2 1997-01-02 00:00:00 27.9 29.7 29.2 28.3 28.5 28.4 NaN 28.5\r## 3 1997-01-03 00:00:00 27.9 29.5 29.1 28.4 28.5 28.3 NaN 28.5\r## 4 1997-01-04 00:00:00 28.0 29.2 29.0 28.6 28.5 28.4 NaN 28.7\r## 5 1997-01-05 00:00:00 28.1 28.8 29.0 28.8 28.7 28.6 NaN 28.7\r## 6 1997-01-06 00:00:00 28.1 28.7 29.0 28.8 28.6 28.6 NaN 28.8\r## 7 1997-01-07 00:00:00 28.1 29.1 29.0 28.8 28.4 28.5 NaN 28.7\r## 8 1997-01-08 00:00:00 28.2 29.0 29.0 28.8 28.4 28.0 NaN 28.4\r## 9 1997-01-09 00:00:00 28.2 29.0 29.0 28.9 28.3 28.4 NaN 28.1\r## 10 1997-01-10 00:00:00 28.2 29.1 28.8 28.9 28.3 28.4 NaN 28.3\r## # ... with 356 more rows, and 13 more variables: `2005` \u0026lt;dbl\u0026gt;, `2006` \u0026lt;dbl\u0026gt;,\r## # `2007` \u0026lt;dbl\u0026gt;, `2008` \u0026lt;dbl\u0026gt;, `2009` \u0026lt;dbl\u0026gt;, `2010` \u0026lt;dbl\u0026gt;, `2011` \u0026lt;dbl\u0026gt;,\r## # `2012` \u0026lt;dbl\u0026gt;, `2013` \u0026lt;dbl\u0026gt;, `2014` \u0026lt;dbl\u0026gt;, `2015` \u0026lt;dbl\u0026gt;, `2016` \u0026lt;dbl\u0026gt;,\r## # `2017` \u0026lt;dbl\u0026gt;\rPython, Pandas and Time\rA time series is a series of data points, which are listed (or indexed) in time order. Usually, a time series is a sequence of values, which are equally spaced points in time. Everything which consists of measured data connected with the corresponding time can be seen as a time series. Measurements can be taken irregularly, but in most cases time series consist of fixed frequencies. This means that data is measured or taken in a regular pattern, i.e. for example every 5 milliseconds, every 10 seconds, or very hour. Often time series are plotted as line charts.\nIn this post of our tutorial on Python with Pandas, we introduce the tools from Pandas dealing with time series. You will learn how to cope with large time series and how modify time series.\nBefore you continue reading it might be useful to go through our tutorial on the standard Python modules dealing with time processing, i.e. datetime, time and calendar:\nimport numpy as np\rimport pandas as pd\rfrom datetime import datetime, timedelta as delta\r\rcreate Date Ranges\rThe date_range function of the pandas library can be used to generate a `DatetimeIndex;\nindex = pd.date_range(\u0026#39;12/24/1970\u0026#39;, \u0026#39;01/03/1971\u0026#39;)\rindex\r## DatetimeIndex([\u0026#39;1970-12-24\u0026#39;, \u0026#39;1970-12-25\u0026#39;, \u0026#39;1970-12-26\u0026#39;, \u0026#39;1970-12-27\u0026#39;,\r## \u0026#39;1970-12-28\u0026#39;, \u0026#39;1970-12-29\u0026#39;, \u0026#39;1970-12-30\u0026#39;, \u0026#39;1970-12-31\u0026#39;,\r## \u0026#39;1971-01-01\u0026#39;, \u0026#39;1971-01-02\u0026#39;, \u0026#39;1971-01-03\u0026#39;],\r## dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;D\u0026#39;)\rWe have passed a start and an end date to date_rangein our previous example. It is also possible to pass only a start or an end date to the function. In this case, we have to determine the number of periods to generate by setting the keyword parameter ‘periods’:\nindex = pd.date_range(\u0026#39;01/01/2018\u0026#39;,periods=365)\rindex\r## DatetimeIndex([\u0026#39;2018-01-01\u0026#39;, \u0026#39;2018-01-02\u0026#39;, \u0026#39;2018-01-03\u0026#39;, \u0026#39;2018-01-04\u0026#39;,\r## \u0026#39;2018-01-05\u0026#39;, \u0026#39;2018-01-06\u0026#39;, \u0026#39;2018-01-07\u0026#39;, \u0026#39;2018-01-08\u0026#39;,\r## \u0026#39;2018-01-09\u0026#39;, \u0026#39;2018-01-10\u0026#39;,\r## ...\r## \u0026#39;2018-12-22\u0026#39;, \u0026#39;2018-12-23\u0026#39;, \u0026#39;2018-12-24\u0026#39;, \u0026#39;2018-12-25\u0026#39;,\r## \u0026#39;2018-12-26\u0026#39;, \u0026#39;2018-12-27\u0026#39;, \u0026#39;2018-12-28\u0026#39;, \u0026#39;2018-12-29\u0026#39;,\r## \u0026#39;2018-12-30\u0026#39;, \u0026#39;2018-12-31\u0026#39;],\r## dtype=\u0026#39;datetime64[ns]\u0026#39;, length=365, freq=\u0026#39;D\u0026#39;)\rwe create a time frequency which contains the month ends between two dates. We can see that the year 2016 contained the 29th of February, because it was a leap year:\nindex = pd.date_range(\u0026#39;01/01/2016\u0026#39;,\u0026#39;01/01/2018\u0026#39;, freq=\u0026quot;M\u0026quot;)\rindex\r## DatetimeIndex([\u0026#39;2016-01-31\u0026#39;, \u0026#39;2016-02-29\u0026#39;, \u0026#39;2016-03-31\u0026#39;, \u0026#39;2016-04-30\u0026#39;,\r## \u0026#39;2016-05-31\u0026#39;, \u0026#39;2016-06-30\u0026#39;, \u0026#39;2016-07-31\u0026#39;, \u0026#39;2016-08-31\u0026#39;,\r## \u0026#39;2016-09-30\u0026#39;, \u0026#39;2016-10-31\u0026#39;, \u0026#39;2016-11-30\u0026#39;, \u0026#39;2016-12-31\u0026#39;,\r## \u0026#39;2017-01-31\u0026#39;, \u0026#39;2017-02-28\u0026#39;, \u0026#39;2017-03-31\u0026#39;, \u0026#39;2017-04-30\u0026#39;,\r## \u0026#39;2017-05-31\u0026#39;, \u0026#39;2017-06-30\u0026#39;, \u0026#39;2017-07-31\u0026#39;, \u0026#39;2017-08-31\u0026#39;,\r## \u0026#39;2017-09-30\u0026#39;, \u0026#39;2017-10-31\u0026#39;, \u0026#39;2017-11-30\u0026#39;, \u0026#39;2017-12-31\u0026#39;],\r## dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;M\u0026#39;)\r\rTime Series in Pandas and Python\rWe could define a Pandas Series, which is built with an index consisting of time stamps.\n\rdates = pd.date_range(\u0026#39;2018-02-25\u0026#39;, \u0026#39;2018-12-02\u0026#39;, freq=\u0026quot;M\u0026quot;)\rvalues = [25, 50, 15, 67, 70, 9, 28, 30, 32, 12]\rts = pd.Series(values, index=dates)\rLet’s check the type of the newly created time series:\ntype(ts)\r## \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt;\rWhat does the index of a time series look like? Let’s see:\nts.index\r## DatetimeIndex([\u0026#39;2018-02-28\u0026#39;, \u0026#39;2018-03-31\u0026#39;, \u0026#39;2018-04-30\u0026#39;, \u0026#39;2018-05-31\u0026#39;,\r## \u0026#39;2018-06-30\u0026#39;, \u0026#39;2018-07-31\u0026#39;, \u0026#39;2018-08-31\u0026#39;, \u0026#39;2018-09-30\u0026#39;,\r## \u0026#39;2018-10-31\u0026#39;, \u0026#39;2018-11-30\u0026#39;],\r## dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;M\u0026#39;)\rWe will create now another time series:\nvalues2 = [32, 54, 18, 61, 72, 19, 21, 33, 29, 17]\rts2 = pd.Series(values2, index=dates)\rIt is possible to use arithmetic operations on time series like we did with other series. We can for example add the two previously created time series:\nts + ts2\r## 2018-02-28 57\r## 2018-03-31 104\r## 2018-04-30 33\r## 2018-05-31 128\r## 2018-06-30 142\r## 2018-07-31 28\r## 2018-08-31 49\r## 2018-09-30 63\r## 2018-10-31 61\r## 2018-11-30 29\r## Freq: M, dtype: int64\rArithmetic mean between both Series, i.e. the values of the series:\n(ts + ts2) /2\r## 2018-02-28 28.5\r## 2018-03-31 52.0\r## 2018-04-30 16.5\r## 2018-05-31 64.0\r## 2018-06-30 71.0\r## 2018-07-31 14.0\r## 2018-08-31 24.5\r## 2018-09-30 31.5\r## 2018-10-31 30.5\r## 2018-11-30 14.5\r## Freq: M, dtype: float64\rts_df =pd.DataFrame(ts2, index=dates, columns={\u0026quot;Temperature\u0026quot;})\rfig, axes = plt.subplots(nrows=1, ncols=1)\rplt.plot(ts_df.index, ts_df.Temperature)\rplt.ylabel(\u0026quot;Temperature\u0026quot;)\r# plt.xticks(ticks=[])\rplt.show()\r\rfig, axes = plt.subplots(nrows=1, ncols=1)\rsns.lineplot(x=ts_df.index, y=ts_df.Temperature)\rplt.show()\rsst = pd.read_excel(\u0026quot;c:/Users/Semba/Documents/kuguru.xlsx\u0026quot;)\rsst.columns;sst.tail()\r## Index([\u0026#39;chl\u0026#39;, \u0026#39;sst\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;month\u0026#39;, \u0026#39;year\u0026#39;], dtype=\u0026#39;object\u0026#39;)\r## chl sst time month year\r## 145 0.217 25.398 2014-08-01 8 2014\r## 146 0.202 25.608 2014-09-01 9 2014\r## 147 0.157 26.395 2014-10-01 10 2014\r## 148 0.108 28.037 2014-11-01 11 2014\r## 149 0.087 29.215 2014-12-01 12 2014\rax = sst.plot(x = \u0026quot;time\u0026quot;, y = \u0026quot;chl\u0026quot;, legend = False)\rax2 = ax.twinx()\rsst.plot(x = \u0026quot;time\u0026quot;, y = \u0026quot;sst\u0026quot;, legned = False, color = \u0026quot;r\u0026quot;)\rax.figure.legend()\rplt.show()\rdf = pd.DataFrame({\u0026quot;date\u0026quot;: [\u0026quot;2018-01-01\u0026quot;, \u0026quot;2018-01-02\u0026quot;, \u0026quot;2018-01-03\u0026quot;, \u0026quot;2018-01-04\u0026quot;],\r\u0026quot;column1\u0026quot;: [555,525,532,585], \u0026quot;column2\u0026quot;: [50,48,49,51]})\rax = df.plot(x=\u0026quot;date\u0026quot;, y=\u0026quot;column1\u0026quot;, legend=False)\rax2 = ax.twinx()\rdf.plot(x=\u0026quot;date\u0026quot;, y=\u0026quot;column2\u0026quot;, ax=ax2, legend=False, color=\u0026quot;r\u0026quot;)\rax.figure.legend()\rplt.show()\rsns.lineplot(x=sst.time, y=sst.chl)\rplt.show()\rWe are interested with how temperature and chlorophyll vary over months. we use the groupby function from pandas to split the months, apply, and combine values by computing the median values\nsst_month = sst[{\u0026#39;chl\u0026#39;, \u0026#39;sst\u0026#39;, \u0026#39;month\u0026#39;}].groupby(\u0026quot;month\u0026quot;).median()\rfig, axes = plt.subplots(nrows=1, ncols=1)\rax = sns.lineplot(x=sst_month.index, y=sst_month.sst, color =\u0026#39;red\u0026#39;, legend=False)\rax2 = ax.twinx()\rsns.lineplot(x=sst_month.index, y=sst_month.chl, ax = ax2, color = \u0026quot;blue\u0026quot;, legend=False)\rplt.ylabel(\u0026quot;Chlorophyll\u0026quot;)\r# ax.figure.legend()\rplt.show()\rpy$sst_month %\u0026gt;% as_tibble() %\u0026gt;% mutate(months = 1:12) %\u0026gt;% pivot_longer(cols = 1:2, names_to = \u0026quot;variable\u0026quot;) %\u0026gt;% ungroup() %\u0026gt;% ggplot() + geom_path(aes(x = months, y = value))+facet_wrap(~variable, scales = \u0026quot;free_y\u0026quot;)+\rscale_x_continuous(breaks = 1:12, labels = seq(lubridate::dmy(\u0026quot;010119\u0026quot;), lubridate::dmy(\u0026quot;311219\u0026quot;), by = \u0026quot;month\u0026quot;) %\u0026gt;% lubridate::month(label = TRUE, abbr = TRUE))\r\rsns.set()\rsns.set_context(\u0026#39;talk\u0026#39;)\rsst_year = sst[{\u0026#39;chl\u0026#39;, \u0026#39;sst\u0026#39;, \u0026#39;year\u0026#39;}].groupby(\u0026quot;year\u0026quot;).median()\rfig, axes = plt.subplots(nrows=1, ncols=1)\rax = sns.lineplot(x=sst_year.index, y=sst_year.sst, color =\u0026#39;red\u0026#39;, legend=False)\rax2 = ax.twinx()\rsns.lineplot(x=sst_year.index, y=sst_year.chl, ax = ax2, color = \u0026quot;blue\u0026quot;, legend=False)\rplt.ylabel(\u0026quot;Chlorophyll\u0026quot;)\r# ax.figure.legend()\rplt.show()\rfig, axes = plt.subplots(nrows=1, ncols=1)\rdata = np.random.rand(40, 60)\rsns.heatmap(data=data)\rplt.show()\r\rheatmaps with seaborn module\rchl_mafia = pd.read_csv(\u0026quot;e:/Data Manipulation/rpy/chl_mafia.csv\u0026quot;)\rchl_mafia_data = pd.pivot_table(data=chl_mafia, values=\u0026quot;chl\u0026quot;, index=\u0026#39;lat\u0026#39;, columns=\u0026#39;lon\u0026#39;)\rfig, axes = plt.subplots(nrows=1, ncols=1)\rsns.heatmap(data=chl_mafia_data, cmap = \u0026quot;jet\u0026quot;, xticklabels=False, yticklabels=False)\rplt.gca().invert_yaxis()\rplt.xlabel([39.4,39.6,39.8,40.0])\rplt.show()\rpy$chl_mafia %\u0026gt;% as_tibble() %\u0026gt;% ggplot() +\rgeom_raster(aes(x = lon, y = lat, fill = chl))+\rscale_fill_gradientn(colours = oce::oce.colors9A(120))\rabcdefghijklmnpqrstuvwxyz\n\r","permalink":"/post/make-use-both-of-r-and-python-languages-for-efficient-data-science/","tags":["Python","R","R Markdown","data science"],"title":"Make use both of R and Python languages for efficient data science"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","permalink":"/author/john-doe/","tags":null,"title":"John Doe"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","permalink":"/author/mark-dinn/","tags":null,"title":"Mark Dinn"}]
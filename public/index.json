[{"categories":[],"contents":"\rA recent findings proved the hard way we human h…\n","permalink":"/projects/the-way-we-make-life/","tags":[],"title":"The way we make life"},{"categories":[],"contents":"\rToday we will learn about Cartopy, one of the most common packages for making maps within python, Another popular and powerful library is Basemap; however, Basemap is going away and being replaced with Cartopy in the near future, For this reason, investing your time in learning mapping in python with Cartopy module is recommended.\nWe thank Research in Computing Earth Sciences because most of material in this post are gleaned from their website. I also thank Phil Elson, a developer of Cartopy and created excellent Cartopy Tutorial\nCartopy makes use of the powerful PROJ.4, numpy and shapely libraries and includes a programatic interface built on top of Matplotlib for the creation of publication quality maps. Key features of cartopy are its object oriented projection definitions, and its ability to transform points, lines, vectors, polygons and images between those projections.\nCartopy Projections and other reference systems\rIn Cartopy, each projection is a class. Most classes of projection can be configured in projection-specific ways, although Cartopy takes an opinionated stance on sensible defaults. Let’s create a Plate Carree projection instance.To do so, we need cartopy’s crs module. This is typically imported as ccrs (Cartopy Coordinate Reference Systems).\nBefore we import the modules we need for mapping, we first have to initialize the linkage of python in Rstudio using the reticulate package and also set the environment in which the seesion will fetch python functions and package. You must load the reticulate package and set the enviroment while in R chunk;\nrequire(reticulate)\r## Loading required package: reticulate\ruse_python(\u0026quot;c:/Python/Anaconda3/\u0026quot;)\rWe then import some Python’s modules using the import function. Make sure that you insert the Pyhon chunk for you to load these modules\n\rimport numpy as np\rimport pandas as pd\rimport cartopy.crs as ccrs\rimport cartopy\rimport matplotlib.pyplot as plt\rWe can access the Mollweide projection with the code chunk below;\nccrs.Mollweide()\r## \u0026lt;cartopy.crs.Mollweide object at 0x0000000024B6F468\u0026gt;\r\rDrawing a map\rCartopy optionally depends upon matplotlib, and each projection knows how to create a matplotlib Axes (or AxesSubplot) that can represent itself.\nThe Axes that the projection creates is a cartopy.mpl.geoaxes.GeoAxes. This Axes subclass overrides some of matplotlib’s existing methods, and adds a number of extremely useful ones for drawing maps.\nWe’ll go back and look at those methods shortly, but first, let’s actually see the cartopy+matplotlib dance in action:\nimport matplotlib.pyplot as plt\rplt.axes(projection=ccrs.PlateCarree())\rplt.show()\rThat was a little underwhelming, but we can see that the Axes created is indeed one of those GeoAxes[Subplot] instances.\nOne of the most useful methods that this class adds on top of the standard matplotlib Axes class is the coastlines method. With no arguments, it will add the Natural Earth 1:110,000,000 scale coastline data to the map.\n\rplt.figure()\rax = plt.axes(projection=ccrs.PlateCarree())\rax.coastlines()\rplt.show()\rWe could just as equally created a matplotlib subplot with one of the many approaches that exist. For example, the plt.subplots function could be used:\n\rfig, ax = plt.subplots(subplot_kw={\u0026#39;projection\u0026#39;: ccrs.PlateCarree()})\rax.coastlines()\rplt.show\r## \u0026lt;function make_python_function.\u0026lt;locals\u0026gt;.python_function at 0x0000000022C13798\u0026gt;\rProjection classes have options we can use to customize the map so that Africa is at the center\nax = plt.axes(projection=ccrs.PlateCarree(central_longitude=0))\rax.coastlines()\rplt.show()\r\rUseful methods of a GeoAxes\rThe cartopy.mpl.geoaxes.GeoAxes class adds a number of useful methods. Let’s take a look at:\nset_global - zoom the map out as much as possible\rset_extent - zoom the map to the given bounding box\rgridlines - add a graticule (and optionally labels) to the axes\rcoastlines - add Natural Earth coastlines to the axes\rstock_img - add a low-resolution Natural Earth background image to the axes\rimshow - add an image (numpy array) to the axes\radd_geometries - add a collection of geometries (Shapely) to the axes\r\rSome More Examples of Different Global Projections\n\rprojections = [ccrs.PlateCarree(),\rccrs.Robinson(),\rccrs.Mercator(),\rccrs.Orthographic(),\rccrs.InterruptedGoodeHomolosine()\r]\rfor proj in projections:\rplt.figure()\rax = plt.axes(projection=proj)\rax.stock_img()\rax.coastlines()\rax.set_title(f\u0026#39;{type(proj)}\u0026#39;)\rplt.show()\r\rRegional Maps\rTo create a regional map, we use the set_extent method of GeoAxis to limit the size of the region.\ncentral_lon =43, central_lat = -8.5\rextent = [35, 50, -16.5, 0]\rax = plt.axes(projection=ccrs.PlateCarree(central_lon, central_lat))\rax.set_extent(extent)\rax.gridlines()\rax.coastlines(resolution=\u0026#39;50m\u0026#39;)\rplt.show()\r\rAdding Features to the Map\rTo give our map more styles and details, we add cartopy.feature objects. Many useful features are built in. These “default features” are at coarse (110m) resolution.\ncartopy.feature.BORDERS Country boundaries\rcartopy.feature.COASTLINE Coastline, including major islands\rcartopy.feature.LAKES Natural and artificial lakes\rcartopy.feature.LAND Land polygons, including major islands\rcartopy.feature.OCEAN Ocean polygons\rcartopy.feature.RIVERS Single-line drainages, including lake centerlines\rcartopy.feature.STATES (limited to the United States at this scale)\r\rimport cartopy.feature as cfeature\rimport numpy as np\rcentral_lat = 37.5\rcentral_lon = -96\rextent = [28, 45, -25, 2]\rcentral_lon = np.mean(extent[:2])\rcentral_lat = np.mean(extent[2:])\rplt.figure(figsize=(6, 6))\rax = plt.axes(projection=ccrs.EquidistantConic(central_lon, central_lat))\rax.set_extent(extent)\rax.add_feature(cartopy.feature.OCEAN)\rax.add_feature(cartopy.feature.LAND, edgecolor=\u0026#39;black\u0026#39;)\rax.add_feature(cartopy.feature.LAKES, edgecolor=\u0026#39;black\u0026#39;)\rax.add_feature(cartopy.feature.RIVERS)\rax.gridlines()\r# ax.gridlines(draw_labels=True, xlocs=[32, 36, 40, 44])\r## \u0026lt;cartopy.mpl.gridliner.Gridliner object at 0x0000000026E0AD48\u0026gt;\rplt.show()\rrivers_50m = cfeature.NaturalEarthFeature(\u0026#39;physical\u0026#39;, \u0026#39;rivers_lake_centerlines\u0026#39;, \u0026#39;10m\u0026#39;)\rcentral_lat = 37.5\rcentral_lon = -96\rextent = [28, 45, -25, 2]\rcentral_lon = np.mean(extent[:2])\rcentral_lat = np.mean(extent[2:])\rplt.figure(figsize=(6, 6))\rax = plt.axes(projection=ccrs.EquidistantConic(central_lon, central_lat))\rax.set_extent(extent)\rax.add_feature(cartopy.feature.OCEAN)\rax.add_feature(cartopy.feature.LAND, edgecolor=\u0026#39;black\u0026#39;)\rax.add_feature(cartopy.feature.LAKES, edgecolor=\u0026#39;black\u0026#39;)\rax.add_feature(rivers_50m, facecolor=\u0026#39;None\u0026#39;, edgecolor=\u0026#39;b\u0026#39;)\rax.gridlines()\r## \u0026lt;cartopy.mpl.gridliner.Gridliner object at 0x0000000027E9E688\u0026gt;\rplt.show()\r\rPlotting 2D (Raster) Data\rThe same principles apply to 2D data. Below we create some example data defined in regular lat / lon coordinates. for this case we will load the global sea surface temperature data. The data is stored as netcdf format and hence we need to load the\n\rimport netCDF4 as nc\rThen we a Dataset function from the netCDF4 module to read the file\nsst = nc.Dataset(\u0026quot;e:/MatlabWorking/GHRSST/20150101.nc\u0026quot;)\rWe need to extract different variables that are stored in the file. But before we extract them, we must look on the internal structure of the file and identify the variables with correct names. We can do that using the nc.variables function\nsst.variables\rWe noticed that the file is the array of lon. lat, analysed_sst, and time. The time is the single interval. Then, we extract the variables as the chunk below shows;\ntime = sst.variables[\u0026#39;time\u0026#39;]\rlon = sst.variables[\u0026#39;lon\u0026#39;]\rlat = sst.variables[\u0026#39;lat\u0026#39;]\rdata = sst.variables[\u0026#39;analysed_sst\u0026#39;]\rBecause the data is in the rectangular grid, we also need to convert the lon and lat to rectangular grid with np.meshgrid(). The purpose of meshgrid is to create a rectangular grid out of an array of x values and an array of y values\n\rlon2d, lat2d = np.meshgrid(lon, lat)\rBecause the temperature was recorded in Kelvin scale, we can simpy convert to degree by simply substracting with 273\n\rdatar = data[0,:,:]-273\r# datar = np.flipud(datar)\rThen we can map the spatial distribution of sea surface temperature around the global as shown in figure 1\n\rplt.figure(figsize=(6,5))\rax = plt.axes(projection=ccrs.PlateCarree())\rax.set_global()\r# ax.set_extent([-170,170,-30,30])\rax.coastlines()\rax.contourf(lon2d, lat2d, datar, cmap = \u0026quot;jet\u0026quot;)\r# ax.gridlines()\r# ax.gridlines(draw_labels=True, xlocs=[32, 36, 40, 44])\r## \u0026lt;matplotlib.contour.QuadContourSet object at 0x000000002C40CE48\u0026gt;\rplt.show()\r\rFigure 1: Sea surface temperature\r\r\r","permalink":"/post/mapping-with-cartopy-in-python/","tags":["data science","mapping","Masumbuko Semba","cartopy"],"title":"Mapping with Cartopy in Python"},{"categories":[],"contents":"\rDuring the last couple of decades, Matlab has been the most commonly-used scripting language in physical oceanography, and it has a large user base in many other fields— however, Python has been gaining ground, often being adopted by former Matlab users as well as by newcomers. Here is a little background to help you understand this shift, and why we advocate using Python from the start.\nPython was designed by a computer scientist as a general–purpose scripting language for easy adoption and widespread use. People tried it and liked it, and the result is that it is widely used throughout the software world, for all sorts of tasks, large and small. There is a vast array of Python packages that are freely available to do all sorts of things—including the sorts of things that oceanographers and other scientists do; but these packages are not neatly bound up in a single product, and the documentation for the language itself and for the packages is similarly scattered and of varying quality.\nWhy use Python instead of Matlab?\rPython is fundamentally a better computer language in many ways.\nIt is suitable for a wider variety of tasks.\rIt scales better from the shortest of scripts to large software projects.\rIt facilitates writing clearer and more concise code.\rWith associated tools, it makes for easier access to existing high-performance codes in compiled languages, and for using smaller pieces of compiled code to speed up critical sections.\rBecause Python is Free and Open Source Software (FOSS), you can install it on any machine without having to deal with a license manager.\rFor the same reason, Python code that is part of a research project can be run by anyone, anywhere, to verify or extend the results.\rMost Python packages you are likely to want to use are developed in an open environment. The scientific Python ecosystem is dynamic and friendly.\r\rWhat are the potential disadvantages?\nInstallation of all the packages one needs can take time and expertise; but distributions like Anaconda, combined with other improvements in python packaging software and repositories, are rapidly solving this problem.\rAlthough progress is being made, the scientific Python stack is not as uniformly well-documented as Matlab; it might take longer to figure out how to do something in Python. You might also find that a routine available in Matlab is not yet available in a Python package.\rMatlab is still mainstream in oceanography–at least among many of the old guard; with Python, you are an early adopter. (If you have a spirit of adventure, this might be considered an advantage.)\r\rWhat about R?\rThe R language (and its commercial counterpart, S) is specialized for statistical calculations and plots. It is widely used in the social sciences but it is less suitable than Python for general computing in oceanography. If one has a need for some of its statistical capabilities, they can be accessed from Python using the interface module, Rpy2.\nrequire(reticulate)\r## Loading required package: reticulate\ruse_python(\u0026quot;c:/Python/Anaconda3/\u0026quot;)\rimport netCDF4 as nc\rimport matplotlib.pyplot as plt\rimport numpy as np\rimport pandas as pd\rimport seaborn as sns\r# from mpl_toolkits.basemap import Basemap\rsst = nc.Dataset(\u0026quot;e:/MatlabWorking/GHRSST/20150101.nc\u0026quot;)\rsst.variables\r## {\u0026#39;lat\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## float32 lat(lat)\r## long_name: latitude\r## standard_name: latitude\r## axis: Y\r## units: degrees_north\r## comment: uniform grid from -89.875 to 89.875 by 0.25\r## unlimited dimensions: ## current shape = (720,)\r## filling on, default _FillValue of 9.969209968386869e+36 used, \u0026#39;lon\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## float32 lon(lon)\r## long_name: longitude\r## standard_name: longitude\r## axis: X\r## units: degrees_east\r## comment: uniform grid from -179.875 to 179.875 by 0.25\r## unlimited dimensions: ## current shape = (1440,)\r## filling on, default _FillValue of 9.969209968386869e+36 used, \u0026#39;time\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int32 time(time)\r## long_name: reference time of sst field\r## standard_name: time\r## axis: T\r## units: seconds since 1981-01-01 00:00:00\r## unlimited dimensions: ## current shape = (1,)\r## filling on, default _FillValue of -2147483647 used, \u0026#39;analysed_sst\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int16 analysed_sst(time, lat, lon)\r## long_name: analysed sea surface temperature\r## standard_name: sea_surface_temperature\r## type: foundation\r## units: kelvin\r## _FillValue: -32768\r## add_offset: 273.15\r## scale_factor: 0.01\r## valid_min: -300\r## valid_max: 4500\r## unlimited dimensions: ## current shape = (1, 720, 1440)\r## filling on, \u0026#39;analysis_error\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int16 analysis_error(time, lat, lon)\r## long_name: estimated error standard deviation of analysed_sst\r## units: kelvin\r## _FillValue: -32768\r## add_offset: 0.0\r## scale_factor: 0.01\r## valid_min: 0\r## valid_max: 127\r## unlimited dimensions: ## current shape = (1, 720, 1440)\r## filling on, \u0026#39;mask\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int8 mask(time, lat, lon)\r## long_name: sea/land field composite mask\r## _FillValue: -128\r## flag_values: 1\r## flag_meanings: sea land lake ice\r## comment: b0:1=grid cell is open sea water b1:1=land is present in this grid cell b2:1=lake surface is present in this grid cell b3:1=sea ice is present in this grid cell b4-b7:reserve for future grid mask data\r## unlimited dimensions: ## current shape = (1, 720, 1440)\r## filling on, \u0026#39;sea_ice_fraction\u0026#39;: \u0026lt;class \u0026#39;netCDF4._netCDF4.Variable\u0026#39;\u0026gt;\r## int8 sea_ice_fraction(time, lat, lon)\r## long_name: sea ice area fraction\r## standard_name: sea ice area fraction\r## units: percent\r## _FillValue: -128\r## add_offset: 0.0\r## scale_factor: 0.01\r## valid_min: 0\r## valid_max: 100\r## unlimited dimensions: ## current shape = (1, 720, 1440)\r## filling on}\rtime = sst.variables[\u0026#39;time\u0026#39;]\rlon = sst.variables[\u0026#39;lon\u0026#39;]\rlat = sst.variables[\u0026#39;lat\u0026#39;]\rdata = sst.variables[\u0026#39;analysed_sst\u0026#39;]\rdata.dimensions\r## (\u0026#39;time\u0026#39;, \u0026#39;lat\u0026#39;, \u0026#39;lon\u0026#39;)\rfig, axes = plt.subplots(nrows=1, ncols=1)\rplt.imshow(data[0,:,:]-273,)\rplt.colorbar()\r## \u0026lt;matplotlib.colorbar.Colorbar object at 0x0000000027C68F88\u0026gt;\rplt.show()\rWe noticed that the data is flipped upside down. we need to correct and map it in the correct orientation. we use np.flipud function from numpy module for correcting the orientation\n\rdatar = np.flipud(data[0,:,:]-273)\rfig, axes = plt.subplots(nrows=1, ncols=1)\rplt.imshow(datar, cmap = \u0026quot;jet\u0026quot;)\rplt.colorbar()\r## \u0026lt;matplotlib.colorbar.Colorbar object at 0x000000002BF3ADC8\u0026gt;\rplt.clim(5,30)\rplt.show()\r# import dependencies\rimport os\ros.environ[\u0026#39;PROJ_LIB\u0026#39;] = \u0026#39;C:/Python/Anaconda3/Lib/site-packages/mpl_toolkits/basemap\u0026#39;\r# note that Basemap will be supported until 2020 only\rfrom mpl_toolkits.basemap import Basemap\rimport matplotlib.pyplot as plt\rimport numpy as np\r# make sure to set your plot size before you start rendering to screen\rplt.figure(figsize=(8, 8))\r# by default, only crude and low resolutions are installed, if you wanted\r# higher fidelity images, refer to the documentation.\rdefault_map = Basemap(projection=\u0026#39;ortho\u0026#39;, lat_0=45, lon_0=-105,\rresolution=\u0026#39;l\u0026#39;, area_thresh=1000.0)\rdefault_map.drawcoastlines()\rdefault_map.drawcountries() # matplotlib provides color creation default_map.fillcontinents(color=\u0026#39;navajowhite\u0026#39;)\rdefault_map.drawmapboundary()\rplt.show()\rread data from local file as pandas data frame and print the five records\n\r## scot = pd.read_csv(\u0026quot;e:/Data Manipulation/rpy/scottish_hills.csv\u0026quot;)\rscot.head()\r## Hill Name Height Latitude Longitude Osgrid\r## 0 A\u0026#39; Bhuidheanach Bheag 936.0 56.870342 -4.199001 NN660775\r## 1 A\u0026#39; Chailleach 997.0 57.693800 -5.128715 NH136714\r## 2 A\u0026#39; Chailleach 929.2 57.109564 -4.179285 NH681041\r## 3 A\u0026#39; Chraileag (A\u0026#39; Chralaig) 1120.0 57.184186 -5.154837 NH094147\r## 4 A\u0026#39; Ghlas-bheinn 918.0 57.255090 -5.303687 NH008231\rBecause of the error we experience in mapping with the basemap package, I will switch to an aternate module called cartopy. Unfortunately, Cartopy is not installed with Anaconda and you need to install it in your machine. The easiest way to install Cartopy on your own computer is with the package manager conda which you can access by typing into a terminal:\nconda install -c conda-forge cartopy\rThen you can load the modules in the session\n\rimport cartopy.crs as ccrs\rfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\rimport cartopy.feature as cfeature\rThen the chunk below\n\rplt.figure(figsize=(2.5,2.5))\rax = plt.axes(projection=ccrs.Mercator())\rax.coastlines(\u0026#39;10m\u0026#39;)\rax.xaxis.set_visible(True)\rax.yaxis.set_visible(True)\rax.set_yticks([56,57,58,59], crs=ccrs.PlateCarree())\r## [\u0026lt;matplotlib.axis.YTick object at 0x000000002D2A5B48\u0026gt;, \u0026lt;matplotlib.axis.YTick object at 0x000000002D2A0E08\u0026gt;, \u0026lt;matplotlib.axis.YTick object at 0x000000002D28C048\u0026gt;, \u0026lt;matplotlib.axis.YTick object at 0x000000002D2D6208\u0026gt;]\rax.set_xticks([-8, -6, -4, -2], crs=ccrs.PlateCarree())\r## [\u0026lt;matplotlib.axis.XTick object at 0x000000002D2A0288\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x000000002D28C648\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x000000002D28C148\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x000000002D2DC3C8\u0026gt;]\rlon_formatter = LongitudeFormatter(zero_direction_label=True)\rlat_formatter = LatitudeFormatter()\rax.xaxis.set_major_formatter(lon_formatter)\rax.yaxis.set_major_formatter(lat_formatter)\rax.set_extent([-8, -1.5, 55.3, 59])\rplt.scatter(scot[\u0026#39;Longitude\u0026#39;],scot[\u0026#39;Latitude\u0026#39;],\rcolor=\u0026#39;red\u0026#39;, marker=\u0026#39;^\u0026#39;, transform=ccrs.PlateCarree())\rplt.show()\r# plt.savefig(\u0026quot;munros.png\u0026quot;)\r\r","permalink":"/post/python-for-oceeanographers-and-marine-scientists/","tags":["Ocenography","marine","science","data science","Masumbuko Semba","Masumbuko","Semba"],"title":"Python for Oceeanographers and Marine Scientists"},{"categories":[],"contents":"\rdlab organized the second Data Tamasha from 4 to 6 December, 2019, that provide an opportunity to company, goverment organization, students and non-govermental organizationto showcase some of the ground–breaking solution in this digital age era. One of the key points from this Tamasha was the use of data science languages as tools in a toolkit. R, Python… Use them both. Leverage their strengths. Don’t build an “R Shop” or a “Python Shop”. Build a High Performance Data Science Team that capitalizes on the unique strengths of both languages.\nWhen you use multiple languages, you gain the ability to select the best tool for the job. The result is a language harmony that increases the data science team’s efficiency, capability, and productivity. The general idea is to be as flexible as possible so we can leverage the best of both languages within our full-stack data science workflow, which includes:\n\rEfficiently exploring data\rModeling, Cross Validating, and Evaluating Model Quality\rCommunicating data science to make better decisions via traditional reports (Word, PowerPoint, Excel, PDF), web-based reports (HTML), and interactive web-applications (Shiny, Django)\rWe can make a slight modification to the R and Python Strengths visualization to organize it in a logical sequence that leverages the strengths:\rR is selected for exploration because of the tidyverse readability and efficiency\rPython is selected for machine learning because of Scikit Learn machine learning pipeline capability\rR is selected for communication because of the advanced reporting utilities including RMarkdown and Shiny (interactive web apps) and the wonderful ggplot2 visualization package\r\rThe reticulate package includes a Python engine for R Markdown that enables easy interoperabilty between Python and R—the two widely used programming languages for data science. We first load the package in R chunk\nrequire(reticulate)\r## Loading required package: reticulate\rrequire(tidyverse)\r## Loading required package: tidyverse\r## -- Attaching packages ---------------------------------------------- tidyverse 1.3.0 --\r## v ggplot2 3.2.1 v purrr 0.3.3\r## v tibble 2.1.3 v dplyr 0.8.3\r## v tidyr 1.0.0 v stringr 1.4.0\r## v readr 1.3.1 v forcats 0.4.0\r## -- Conflicts ------------------------------------------------- tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rBy default, reticulate package use the version of Python found on your PC PATH. However, if you have more than one version of Python installed in your machine, you can use the use_python function to use an alternative version.\nuse_python(\u0026quot;c:/Python/Anaconda3/\u0026quot;)\rFor use to call the Python libraries, we need to call them from the Python Chunk and not R chunk. Python chunk all execute within a single Python session to have access to all objects created in previous chunk. Like the R chunk, all chunk options works in similar manner in Python chunk. To access python function, we first have to import their corresponding libraries.\n\rimport pandas as pd\rWe use the read_excel() function from pandas library to import the file from the PC to R session\n\rsst = pd.read_excel(\u0026quot;E:/Data Manipulation/Temperature data/processing/Chumbe_SST_Temperature 1997-30Nov2017_IMS_Muhando.xlsx\u0026quot;)\rsst\r## day 1997 1998 ... 2015 2016 2017\r## 0 1997-01-01 27.830167 29.779000 ... 28.914583 28.942917 29.215833\r## 1 1997-01-02 27.889833 29.669333 ... 28.829583 28.886250 29.354375\r## 2 1997-01-03 27.932833 29.495333 ... 28.631250 28.928750 29.420833\r## 3 1997-01-04 28.025000 29.241333 ... 28.517917 28.893333 29.532083\r## 4 1997-01-05 28.058333 28.829000 ... 28.397500 28.935833 29.744792\r## .. ... ... ... ... ... ... ...\r## 361 1997-12-27 29.380139 29.329375 ... 28.850833 29.096458 NaN\r## 362 1997-12-28 29.379722 29.334792 ... 28.709167 28.665625 NaN\r## 363 1997-12-29 29.469583 29.330833 ... 28.872083 28.734792 NaN\r## 364 1997-12-30 29.616111 29.042222 ... 28.964167 28.966458 NaN\r## 365 1997-12-31 29.839583 29.086389 ... 28.900417 29.173750 NaN\r## ## [366 rows x 22 columns]\rimport seaborn as sns\rimport matplotlib.pyplot as plt\r# fig, axes = plt.subplots(nrows=1, ncols=1)\r# fig, axes = plt.subplot(nrows = 1, ncols = 1)\rsns.lineplot(x=\u0026#39;day\u0026#39;, y= \u0026#39;1997\u0026#39;, data=sst)\rplt.show()\rreticulate::py$sst %\u0026gt;% as_tibble()\r## # A tibble: 366 x 22\r## day `1997` `1998` `1999` `2000` `2001` `2002` `2003` `2004`\r## \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1997-01-01 00:00:00 27.8 29.8 29.1 28.3 28.7 28.3 NaN 28.4\r## 2 1997-01-02 00:00:00 27.9 29.7 29.2 28.3 28.5 28.4 NaN 28.5\r## 3 1997-01-03 00:00:00 27.9 29.5 29.1 28.4 28.5 28.3 NaN 28.5\r## 4 1997-01-04 00:00:00 28.0 29.2 29.0 28.6 28.5 28.4 NaN 28.7\r## 5 1997-01-05 00:00:00 28.1 28.8 29.0 28.8 28.7 28.6 NaN 28.7\r## 6 1997-01-06 00:00:00 28.1 28.7 29.0 28.8 28.6 28.6 NaN 28.8\r## 7 1997-01-07 00:00:00 28.1 29.1 29.0 28.8 28.4 28.5 NaN 28.7\r## 8 1997-01-08 00:00:00 28.2 29.0 29.0 28.8 28.4 28.0 NaN 28.4\r## 9 1997-01-09 00:00:00 28.2 29.0 29.0 28.9 28.3 28.4 NaN 28.1\r## 10 1997-01-10 00:00:00 28.2 29.1 28.8 28.9 28.3 28.4 NaN 28.3\r## # ... with 356 more rows, and 13 more variables: `2005` \u0026lt;dbl\u0026gt;, `2006` \u0026lt;dbl\u0026gt;,\r## # `2007` \u0026lt;dbl\u0026gt;, `2008` \u0026lt;dbl\u0026gt;, `2009` \u0026lt;dbl\u0026gt;, `2010` \u0026lt;dbl\u0026gt;, `2011` \u0026lt;dbl\u0026gt;,\r## # `2012` \u0026lt;dbl\u0026gt;, `2013` \u0026lt;dbl\u0026gt;, `2014` \u0026lt;dbl\u0026gt;, `2015` \u0026lt;dbl\u0026gt;, `2016` \u0026lt;dbl\u0026gt;,\r## # `2017` \u0026lt;dbl\u0026gt;\rPython, Pandas and Time\rA time series is a series of data points, which are listed (or indexed) in time order. Usually, a time series is a sequence of values, which are equally spaced points in time. Everything which consists of measured data connected with the corresponding time can be seen as a time series. Measurements can be taken irregularly, but in most cases time series consist of fixed frequencies. This means that data is measured or taken in a regular pattern, i.e. for example every 5 milliseconds, every 10 seconds, or very hour. Often time series are plotted as line charts.\nIn this post of our tutorial on Python with Pandas, we introduce the tools from Pandas dealing with time series. You will learn how to cope with large time series and how modify time series.\nBefore you continue reading it might be useful to go through our tutorial on the standard Python modules dealing with time processing, i.e. datetime, time and calendar:\nimport numpy as np\rimport pandas as pd\rfrom datetime import datetime, timedelta as delta\r\rcreate Date Ranges\rThe date_range function of the pandas library can be used to generate a `DatetimeIndex;\nindex = pd.date_range(\u0026#39;12/24/1970\u0026#39;, \u0026#39;01/03/1971\u0026#39;)\rindex\r## DatetimeIndex([\u0026#39;1970-12-24\u0026#39;, \u0026#39;1970-12-25\u0026#39;, \u0026#39;1970-12-26\u0026#39;, \u0026#39;1970-12-27\u0026#39;,\r## \u0026#39;1970-12-28\u0026#39;, \u0026#39;1970-12-29\u0026#39;, \u0026#39;1970-12-30\u0026#39;, \u0026#39;1970-12-31\u0026#39;,\r## \u0026#39;1971-01-01\u0026#39;, \u0026#39;1971-01-02\u0026#39;, \u0026#39;1971-01-03\u0026#39;],\r## dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;D\u0026#39;)\rWe have passed a start and an end date to date_rangein our previous example. It is also possible to pass only a start or an end date to the function. In this case, we have to determine the number of periods to generate by setting the keyword parameter ‘periods’:\nindex = pd.date_range(\u0026#39;01/01/2018\u0026#39;,periods=365)\rindex\r## DatetimeIndex([\u0026#39;2018-01-01\u0026#39;, \u0026#39;2018-01-02\u0026#39;, \u0026#39;2018-01-03\u0026#39;, \u0026#39;2018-01-04\u0026#39;,\r## \u0026#39;2018-01-05\u0026#39;, \u0026#39;2018-01-06\u0026#39;, \u0026#39;2018-01-07\u0026#39;, \u0026#39;2018-01-08\u0026#39;,\r## \u0026#39;2018-01-09\u0026#39;, \u0026#39;2018-01-10\u0026#39;,\r## ...\r## \u0026#39;2018-12-22\u0026#39;, \u0026#39;2018-12-23\u0026#39;, \u0026#39;2018-12-24\u0026#39;, \u0026#39;2018-12-25\u0026#39;,\r## \u0026#39;2018-12-26\u0026#39;, \u0026#39;2018-12-27\u0026#39;, \u0026#39;2018-12-28\u0026#39;, \u0026#39;2018-12-29\u0026#39;,\r## \u0026#39;2018-12-30\u0026#39;, \u0026#39;2018-12-31\u0026#39;],\r## dtype=\u0026#39;datetime64[ns]\u0026#39;, length=365, freq=\u0026#39;D\u0026#39;)\rwe create a time frequency which contains the month ends between two dates. We can see that the year 2016 contained the 29th of February, because it was a leap year:\nindex = pd.date_range(\u0026#39;01/01/2016\u0026#39;,\u0026#39;01/01/2018\u0026#39;, freq=\u0026quot;M\u0026quot;)\rindex\r## DatetimeIndex([\u0026#39;2016-01-31\u0026#39;, \u0026#39;2016-02-29\u0026#39;, \u0026#39;2016-03-31\u0026#39;, \u0026#39;2016-04-30\u0026#39;,\r## \u0026#39;2016-05-31\u0026#39;, \u0026#39;2016-06-30\u0026#39;, \u0026#39;2016-07-31\u0026#39;, \u0026#39;2016-08-31\u0026#39;,\r## \u0026#39;2016-09-30\u0026#39;, \u0026#39;2016-10-31\u0026#39;, \u0026#39;2016-11-30\u0026#39;, \u0026#39;2016-12-31\u0026#39;,\r## \u0026#39;2017-01-31\u0026#39;, \u0026#39;2017-02-28\u0026#39;, \u0026#39;2017-03-31\u0026#39;, \u0026#39;2017-04-30\u0026#39;,\r## \u0026#39;2017-05-31\u0026#39;, \u0026#39;2017-06-30\u0026#39;, \u0026#39;2017-07-31\u0026#39;, \u0026#39;2017-08-31\u0026#39;,\r## \u0026#39;2017-09-30\u0026#39;, \u0026#39;2017-10-31\u0026#39;, \u0026#39;2017-11-30\u0026#39;, \u0026#39;2017-12-31\u0026#39;],\r## dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;M\u0026#39;)\r\rTime Series in Pandas and Python\rWe could define a Pandas Series, which is built with an index consisting of time stamps.\n\rdates = pd.date_range(\u0026#39;2018-02-25\u0026#39;, \u0026#39;2018-12-02\u0026#39;, freq=\u0026quot;M\u0026quot;)\rvalues = [25, 50, 15, 67, 70, 9, 28, 30, 32, 12]\rts = pd.Series(values, index=dates)\rLet’s check the type of the newly created time series:\ntype(ts)\r## \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt;\rWhat does the index of a time series look like? Let’s see:\nts.index\r## DatetimeIndex([\u0026#39;2018-02-28\u0026#39;, \u0026#39;2018-03-31\u0026#39;, \u0026#39;2018-04-30\u0026#39;, \u0026#39;2018-05-31\u0026#39;,\r## \u0026#39;2018-06-30\u0026#39;, \u0026#39;2018-07-31\u0026#39;, \u0026#39;2018-08-31\u0026#39;, \u0026#39;2018-09-30\u0026#39;,\r## \u0026#39;2018-10-31\u0026#39;, \u0026#39;2018-11-30\u0026#39;],\r## dtype=\u0026#39;datetime64[ns]\u0026#39;, freq=\u0026#39;M\u0026#39;)\rWe will create now another time series:\nvalues2 = [32, 54, 18, 61, 72, 19, 21, 33, 29, 17]\rts2 = pd.Series(values2, index=dates)\rIt is possible to use arithmetic operations on time series like we did with other series. We can for example add the two previously created time series:\nts + ts2\r## 2018-02-28 57\r## 2018-03-31 104\r## 2018-04-30 33\r## 2018-05-31 128\r## 2018-06-30 142\r## 2018-07-31 28\r## 2018-08-31 49\r## 2018-09-30 63\r## 2018-10-31 61\r## 2018-11-30 29\r## Freq: M, dtype: int64\rArithmetic mean between both Series, i.e. the values of the series:\n(ts + ts2) /2\r## 2018-02-28 28.5\r## 2018-03-31 52.0\r## 2018-04-30 16.5\r## 2018-05-31 64.0\r## 2018-06-30 71.0\r## 2018-07-31 14.0\r## 2018-08-31 24.5\r## 2018-09-30 31.5\r## 2018-10-31 30.5\r## 2018-11-30 14.5\r## Freq: M, dtype: float64\rts_df =pd.DataFrame(ts2, index=dates, columns={\u0026quot;Temperature\u0026quot;})\rfig, axes = plt.subplots(nrows=1, ncols=1)\rplt.plot(ts_df.index, ts_df.Temperature)\rplt.ylabel(\u0026quot;Temperature\u0026quot;)\r# plt.xticks(ticks=[])\rplt.show()\r\rfig, axes = plt.subplots(nrows=1, ncols=1)\rsns.lineplot(x=ts_df.index, y=ts_df.Temperature)\rplt.show()\rsst = pd.read_excel(\u0026quot;c:/Users/Semba/Documents/kuguru.xlsx\u0026quot;)\rsst.columns;sst.tail()\r## Index([\u0026#39;chl\u0026#39;, \u0026#39;sst\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;month\u0026#39;, \u0026#39;year\u0026#39;], dtype=\u0026#39;object\u0026#39;)\r## chl sst time month year\r## 145 0.217 25.398 2014-08-01 8 2014\r## 146 0.202 25.608 2014-09-01 9 2014\r## 147 0.157 26.395 2014-10-01 10 2014\r## 148 0.108 28.037 2014-11-01 11 2014\r## 149 0.087 29.215 2014-12-01 12 2014\rax = sst.plot(x = \u0026quot;time\u0026quot;, y = \u0026quot;chl\u0026quot;, legend = False)\rax2 = ax.twinx()\rsst.plot(x = \u0026quot;time\u0026quot;, y = \u0026quot;sst\u0026quot;, legned = False, color = \u0026quot;r\u0026quot;)\rax.figure.legend()\rplt.show()\rdf = pd.DataFrame({\u0026quot;date\u0026quot;: [\u0026quot;2018-01-01\u0026quot;, \u0026quot;2018-01-02\u0026quot;, \u0026quot;2018-01-03\u0026quot;, \u0026quot;2018-01-04\u0026quot;],\r\u0026quot;column1\u0026quot;: [555,525,532,585], \u0026quot;column2\u0026quot;: [50,48,49,51]})\rax = df.plot(x=\u0026quot;date\u0026quot;, y=\u0026quot;column1\u0026quot;, legend=False)\rax2 = ax.twinx()\rdf.plot(x=\u0026quot;date\u0026quot;, y=\u0026quot;column2\u0026quot;, ax=ax2, legend=False, color=\u0026quot;r\u0026quot;)\rax.figure.legend()\rplt.show()\rsns.lineplot(x=sst.time, y=sst.chl)\rplt.show()\rWe are interested with how temperature and chlorophyll vary over months. we use the groupby function from pandas to split the months, apply, and combine values by computing the median values\nsst_month = sst[{\u0026#39;chl\u0026#39;, \u0026#39;sst\u0026#39;, \u0026#39;month\u0026#39;}].groupby(\u0026quot;month\u0026quot;).median()\rfig, axes = plt.subplots(nrows=1, ncols=1)\rax = sns.lineplot(x=sst_month.index, y=sst_month.sst, color =\u0026#39;red\u0026#39;, legend=False)\rax2 = ax.twinx()\rsns.lineplot(x=sst_month.index, y=sst_month.chl, ax = ax2, color = \u0026quot;blue\u0026quot;, legend=False)\rplt.ylabel(\u0026quot;Chlorophyll\u0026quot;)\r# ax.figure.legend()\rplt.show()\rpy$sst_month %\u0026gt;% as_tibble() %\u0026gt;% mutate(months = 1:12) %\u0026gt;% pivot_longer(cols = 1:2, names_to = \u0026quot;variable\u0026quot;) %\u0026gt;% ungroup() %\u0026gt;% ggplot() + geom_path(aes(x = months, y = value))+facet_wrap(~variable, scales = \u0026quot;free_y\u0026quot;)+\rscale_x_continuous(breaks = 1:12, labels = seq(lubridate::dmy(\u0026quot;010119\u0026quot;), lubridate::dmy(\u0026quot;311219\u0026quot;), by = \u0026quot;month\u0026quot;) %\u0026gt;% lubridate::month(label = TRUE, abbr = TRUE))\r\rsns.set()\rsns.set_context(\u0026#39;talk\u0026#39;)\rsst_year = sst[{\u0026#39;chl\u0026#39;, \u0026#39;sst\u0026#39;, \u0026#39;year\u0026#39;}].groupby(\u0026quot;year\u0026quot;).median()\rfig, axes = plt.subplots(nrows=1, ncols=1)\rax = sns.lineplot(x=sst_year.index, y=sst_year.sst, color =\u0026#39;red\u0026#39;, legend=False)\rax2 = ax.twinx()\rsns.lineplot(x=sst_year.index, y=sst_year.chl, ax = ax2, color = \u0026quot;blue\u0026quot;, legend=False)\rplt.ylabel(\u0026quot;Chlorophyll\u0026quot;)\r# ax.figure.legend()\rplt.show()\rfig, axes = plt.subplots(nrows=1, ncols=1)\rdata = np.random.rand(40, 60)\rsns.heatmap(data=data)\rplt.show()\r\rheatmaps with seaborn module\rchl_mafia = pd.read_csv(\u0026quot;e:/Data Manipulation/rpy/chl_mafia.csv\u0026quot;)\rchl_mafia_data = pd.pivot_table(data=chl_mafia, values=\u0026quot;chl\u0026quot;, index=\u0026#39;lat\u0026#39;, columns=\u0026#39;lon\u0026#39;)\rfig, axes = plt.subplots(nrows=1, ncols=1)\rsns.heatmap(data=chl_mafia_data, cmap = \u0026quot;jet\u0026quot;, xticklabels=False, yticklabels=False)\rplt.gca().invert_yaxis()\rplt.xlabel([39.4,39.6,39.8,40.0])\rplt.show()\rpy$chl_mafia %\u0026gt;% as_tibble() %\u0026gt;% ggplot() +\rgeom_raster(aes(x = lon, y = lat, fill = chl))+\rscale_fill_gradientn(colours = oce::oce.colors9A(120))\rabcdefghijklmnpqrstuvwxyz\n\r","permalink":"/post/make-use-both-of-r-and-python-languages-for-efficient-data-science/","tags":["Python","R","R Markdown","data science"],"title":"Make use both of R and Python languages for efficient data science"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","permalink":"/author/john-doe/","tags":null,"title":"John Doe"},{"categories":null,"contents":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n","permalink":"/author/mark-dinn/","tags":null,"title":"Mark Dinn"}]